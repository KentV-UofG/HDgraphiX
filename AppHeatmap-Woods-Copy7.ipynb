{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420039b-2677-4a5b-a879-5f0363a7747f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, send_file, make_response, Response\n",
    "from io import BytesIO\n",
    "import logging\n",
    "import io\n",
    "import zipfile\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.patheffects as pe\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "import scipy\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import font_manager as fm, rcParams\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "pd.options.display.width = 200\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "app = Flask(__name__, template_folder='templates')\n",
    "app.config['UPLOAD_FOLDER'] = \"uploads\"  # Ensure this folder exists\n",
    "\n",
    "@app.route('/pdf')\n",
    "def pdf_view():\n",
    "    pdf_path = 'images/Logsaso.pdf'\n",
    "    return send_file(pdf_path, as_attachment=False)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "# Set up Azure Blob Storage credentials\n",
    "AZURE_STORAGE_CONNECTION_STRING = \"\"\n",
    "CONTAINER_NAME = \"heatmap1vahidi\"\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) \n",
    "container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "\n",
    "\n",
    "output_bitmap_h_count = 0\n",
    "output_bitmap_v_count = 0\n",
    "obj_id = ''\n",
    "seg_id = ''\n",
    "chain_id = ''\n",
    "chain_dict = {}    \n",
    "\n",
    "@app.route('/generate_pdf', methods=['POST'])\n",
    "def generate_pdf():\n",
    "    timestamp = datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
    " \n",
    "    AZURE_STORAGE_CONNECTION_STRING = \"\"\n",
    "    CONTAINER_NAME = \"heatmap1vahidi\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) \n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    FONT_CONTAINER_NAME = 'uploads'\n",
    "    \n",
    "    fontchoice = float(request.form.get('fontchoiceu', 1))\n",
    "    if fontchoice == 1:\n",
    "        FONT_BLOB_NAME = 'Arial.ttf'\n",
    "        font_path = '/tmp/Arial.ttf'\n",
    "    elif fontchoice == 2: \n",
    "        FONT_BLOB_NAME = 'helvetica.ttf'\n",
    "        font_path = '/tmp/helvetica.ttf'\n",
    "    elif fontchoice == 3:\n",
    "        FONT_BLOB_NAME = 'Times New Roman.ttf'\n",
    "        font_path = '/tmp/Times New Roman.ttf'\n",
    "        \n",
    "    font_container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    blob_client = blob_service_client.get_blob_client(container=FONT_CONTAINER_NAME, blob=FONT_BLOB_NAME)\n",
    "    font_data = blob_client.download_blob()\n",
    "    font_bytes = io.BytesIO(font_data.readall())\n",
    "\n",
    "    with open(font_path, 'wb') as f:\n",
    "        f.write(font_bytes.getvalue())\n",
    "    fm.fontManager.addfont(font_path)\n",
    "\n",
    "    if fontchoice == 1:\n",
    "        plt.rcParams['font.family'] = 'Arial'\n",
    "    elif fontchoice == 2:\n",
    "        plt.rcParams['font.family'] = 'helvetica'\n",
    "    elif fontchoice == 3:\n",
    "        plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    \n",
    "    ###########################################################################################################################\n",
    "    #\n",
    "    # Constants and default values. Do not change this section.\n",
    "    # All of these variables can be set to desired values in the next section.\n",
    "\n",
    "    output_buffer = []\n",
    "    heatmap_buffer = []\n",
    "    buffer = []\n",
    "    \n",
    "    # Set up Azure Blob Storage credentials\n",
    "    AZURE_STORAGE_CONNECTION_STRING = \"\"\n",
    "    CONTAINER_NAME = \"heatmap1vahidi\"\n",
    "    \n",
    "    blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) \n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    \n",
    "    # proton mass\n",
    "    mp = 1.00727647\n",
    "    \n",
    "    output_csv_file  = 'HDX processed data.csv'\n",
    "    output_pdf_file  = 'HDX heatmap.pdf'\n",
    "    \n",
    "    mutation_msg = 1\n",
    "\n",
    "    p_threshold = 0.05\n",
    "    Dd_threshold = 0.5       # Delta D uptake threshold for the vertical line in scatter plot\n",
    "    significant_only = 1\n",
    "    scatter_plot = 2020\n",
    "    scatter_dir = 'scatter_plots'\n",
    "    \n",
    "    split_outp_by_prot = ''\n",
    "    split_outp_chunks = 0\n",
    "    split_outp_by_row = []\n",
    "    \n",
    "    plot_h = 1\n",
    "    plot_v = 0\n",
    "\n",
    "    pymol_print = 1\n",
    "    pymol_dir = 'pymol_macros'\n",
    "\n",
    "    def create_container_if_not_exists(container_client):\n",
    "        try:\n",
    "            # Try to create the container. If it already exists, this will raise an exception.\n",
    "            container_client.create_container()\n",
    "            print(f\"Container '{container_client.container_name}' created successfully.\")\n",
    "        except ResourceExistsError:\n",
    "            # Handle case where container already exists\n",
    "            print(f\"Container '{container_client.container_name}' already exists.\")\n",
    "        except Exception as e:\n",
    "            # Raise any other exceptions\n",
    "            raise\n",
    "\n",
    "    def download_blob_as_bytes(container_client, blob_name):\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        stream = BytesIO()\n",
    "        blob_client.download_blob().download_to_stream(stream)\n",
    "        stream.seek(0)\n",
    "        return stream\n",
    "    \n",
    "    def generate_unique_filename(base_name, extension, timestamp):\n",
    "        unique_filename = f\"{base_name}_{timestamp}.{extension}\"\n",
    "        return unique_filename\n",
    "\n",
    "    def upload_to_blob_storage(container_client, file_content, blob_name):\n",
    "        create_container_if_not_exists(container_client)\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        blob_client.upload_blob(file_content, overwrite=True)\n",
    "        return blob_name\n",
    "        \n",
    "    #\n",
    "    ###########################################################################################################################\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    #Reset Any Changed Values\n",
    "    neg_col = None\n",
    "    pos_col = None\n",
    "    custom_colors = None\n",
    "    num_col = None\n",
    "    num_shades = None\n",
    "    num_inputs = None\n",
    "    bound1 = None\n",
    "    bound2 = None\n",
    "    bound3 = None\n",
    "    bound4 = None\n",
    "    bound5 = None\n",
    "    bound6 = None\n",
    "    bound7 = None\n",
    "    bound8 = None\n",
    "    custom_bounds = None\n",
    "    custb = None\n",
    "    h_or_v =  None\n",
    "    font_size = 36\n",
    "    font_size_title = 48\n",
    "    ao = None\n",
    "    dtime = None\n",
    "    drop_times = None\n",
    "    dpept = None\n",
    "    drop_pept = None\n",
    "    dpeppro = None\n",
    "    dpepst = None\n",
    "    dpepend = None\n",
    "    dprot = None\n",
    "    drop_prot = None\n",
    "    dpro = None\n",
    "    renumdict = None\n",
    "    key1 = None\n",
    "    value1 = None\n",
    "    key2 = None\n",
    "    value2 = None\n",
    "    renumbering_dict = None\n",
    "    iddict = None\n",
    "    mutidWT = None\n",
    "    mutidMUT = None\n",
    "    mut_id_dict = None\n",
    "    mutdict = None\n",
    "    mutdictres = None\n",
    "    mutdictwt = None\n",
    "    mutation_dict = None\n",
    "    slist = None\n",
    "    state1_list = None\n",
    "    state2_list = None\n",
    "    s1 = None\n",
    "    s2 = None\n",
    "    state_list = None\n",
    "    buffer = None\n",
    "    input_csv_file = None\n",
    "    pept_tick_labels = None\n",
    "    time_tick_labels = None\n",
    "    f_pymol = None\n",
    "    zip_file = None\n",
    "    download_pymol = None\n",
    "    obj_id = ''\n",
    "    seg_id = ''\n",
    "    chain_id = ''\n",
    "    chain_dict = {}\n",
    "    output_bitmap_h_count = 0\n",
    "    output_bitmap_v_count = 0\n",
    "    output_bitmap_file = None\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    ###########################################################################################################################\n",
    "    #\n",
    "    # Define a few functions.\n",
    "\n",
    "    # from https://stackoverflow.com/questions/24005221/ipython-notebook-early-exit-from-cell\n",
    "    class StopExecution(Exception):\n",
    "        def _render_traceback_(self):\n",
    "            pass\n",
    "    \n",
    "    # from https://stackoverflow.com/questions/25668828/how-to-create-colour-gradient-in-python\n",
    "    def colorFader(c1,c2,mix=0):   #fade (linear interpolate) from color c1 (at mix=0) to c2 (mix=1)\n",
    "        c1 = c1.strip()\n",
    "        c1=np.array(mc.to_rgb(c1))\n",
    "        c2 = c2.strip()\n",
    "        c2=np.array(mc.to_rgb(c2))\n",
    "        return mc.to_hex((1-mix)*c1 + mix*c2)\n",
    "   \n",
    "    \n",
    "    # A function to format a string\n",
    "    def mk_pymol(a,b,c,d):\n",
    "        #obj_id = 'obj'\n",
    "        #seg_id = 'seg'\n",
    "        chain_id = chain_dict.get(a, 'default_chain')  # Get the chain ID from chain_dict or use a default value\n",
    "        return 'alter /{0}/{1}/{2}/{3}-{4}, b={5:.3f}'.format(obj_id, seg_id, chain_id, b, c, d)\n",
    "\n",
    "    ######THE FOLLOWING FUNCTIONS ARE FOR CONVERTING HDExaminer FILES TO DYNAMX FORMAT#####\n",
    "    \n",
    "    def clean_and_divide_entry(entry):\n",
    "        if entry == 'Full-D':\n",
    "            divided_entry = 1000\n",
    "            return divided_entry\n",
    "        else:\n",
    "            # Remove all letters from the entry\n",
    "            cleaned_entry = re.sub(r'[a-zA-Z]', '', entry)\n",
    "            if cleaned_entry:  # Check if the entry is not empty after removing letters\n",
    "                # Convert to integer and divide by 60\n",
    "                try:\n",
    "                    divided_entry = round(int(cleaned_entry) / 60,3)\n",
    "                    return divided_entry\n",
    "                except ValueError:\n",
    "                    return None  # Return None if conversion to integer fails\n",
    "            return None  # Return None for empty or invalid entries\n",
    "    \n",
    "    def process_first_line(input_file, encoding='utf-8'):\n",
    "        with open(input_file, 'r', encoding=encoding) as file:\n",
    "            reader = csv.reader(file)\n",
    "            first_line = next(reader)  # Read the first line of the CSV\n",
    "            # Clean and divide each entry in the first line\n",
    "            cleaned_and_divided_entries = [clean_and_divide_entry(entry) for entry in first_line if clean_and_divide_entry(entry) is not None]\n",
    "            cleaned_and_divided_entries.insert(0, 0)\n",
    "            #print(cleaned_and_divided_entries)\n",
    "            # Get the number of valid entries\n",
    "            num_entries = len(cleaned_and_divided_entries)\n",
    "            #print(num_entries)\n",
    "            return num_entries, cleaned_and_divided_entries\n",
    "    \n",
    "    def count_rows_after_second(input_file, encoding='utf-8'):\n",
    "        with open(input_file, 'r', encoding=encoding) as file:\n",
    "            reader = csv.reader(file)\n",
    "            next(reader)  # Skip the first line\n",
    "            next(reader)  # Skip the second line\n",
    "            row_count = sum(1 for row in reader if any(row))  # Count non-empty rows\n",
    "            return row_count\n",
    "    \n",
    "    def write_output_csv(output_file, first_row_values, data_rows, repeated_statevalues):\n",
    "        print(f\"Number of RT entries: {len(RTresult)}\")\n",
    "        print(f\"Number of Center entries: {len(center)}\")\n",
    "        print(f\"Number of protlist entries: {len(repeated_protlist)}\")\n",
    "        print(f\"Number of filelist entries: {len(repeated_filelist)}\")\n",
    "        print(f\"Number of charge entries: {len(repeated_charge)}\")\n",
    "        print(f\"Number of start entries: {len(repeated_startvalues)}\")\n",
    "        print(f\"Number of end entries: {len(repeated_endvalues)}\")\n",
    "        print(f\"Number of seq entries: {len(repeated_seqvalues)}\")\n",
    "        print(f\"Number of maxup entries: {len(repeated_MaxUpvalues)}\")\n",
    "        print(f\"Number of state entries: {len(repeated_statevalues)}\")\n",
    "        print(f\"Number of charge entries: {len(repeated_charge)}\")\n",
    "        print(f\"Number of exposure entries: {len(repeated_exposurevalues)}\")\n",
    "        with open(output_file, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            # Write the first row with preset values\n",
    "            writer.writerow(first_row_values)\n",
    "            # Write a spacer row after the headers\n",
    "            writer.writerow([])\n",
    "            # Write the data rows with blank rows as spacers\n",
    "            for i in range(len(data_rows)):\n",
    "                row = [''] * 15  # Create a row with 15 blank values\n",
    "                if i < len(repeated_statevalues):\n",
    "                    row[0] = repeated_protlist[i] #this returns a repeated gen_prot list for the protein column (likely not necessary)\n",
    "                    row[1] = repeated_startvalues[i]\n",
    "                    row[2] = repeated_endvalues[i]\n",
    "                    row[3] = repeated_seqvalues[i]\n",
    "                    #rows 4 & 5 (modification and fragment) not necessary\n",
    "                    row[6] = repeated_MaxUpvalues[i]\n",
    "                    row[7] = center[i] #this just repeats the center value to ensure the file works properly (likely not necessary)\n",
    "                    row[8] = repeated_statevalues[i]\n",
    "                    row[9] = repeated_exposurevalues[i]\n",
    "                    row[10] = repeated_filelist[i] #this returns a repeated gen_file list for the file column (likely not necessary)\n",
    "                    row[11] = repeated_charge[i] #Because HDExaminer returns processed D uptake, want Als script to \"ignore\" charge, treat all as 1\n",
    "                    row[12] = RTresult[i]\n",
    "                    #row 13 is inten in DynamX files. There is no direct comparison in the HDExaminer file format except maybe score, however the scale is vastly different\n",
    "                    row[14] = center[i]\n",
    "                writer.writerow(row)\n",
    "                writer.writerow([])  # Write a blank row as a spacer\n",
    "    \n",
    "    def gather_and_condense_columns(input_file, num_entries):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_file)\n",
    "        # Assuming row 3 is the row of interest (adjust index if necessary)\n",
    "        row_idx = 0  # Python uses zero-based indexing, so row 3 is index 2\n",
    "        # Find all column indices in row 3 where the value matches search_value\n",
    "        search_value = 'Start RT'\n",
    "        # Check row 3 for the presence of search_value\n",
    "        row_3 = df.iloc[row_idx]\n",
    "        #print(row_3)\n",
    "        # Find all column indices in row 3 where the value matches search_value\n",
    "        column_indices = [idx for idx, value in enumerate(row_3) if value == search_value or value == 'Search RT']\n",
    "        # Debug: print column_indices\n",
    "        print(f\"Column indices with '{search_value}' in row 3: {column_indices}\")\n",
    "        #print(col_idx)\n",
    "        # Initialize an empty list to store gathered values\n",
    "        gathered_values = []\n",
    "        # Iterate through each row starting from row 3\n",
    "        for idx in range(row_idx+1, len(df)):  # Start from row 4 onwards (zero-based index)\n",
    "            row_values = []  # Start each row with 0\n",
    "            # Gather all values from identified columns\n",
    "            for col_idx in column_indices:\n",
    "                #print('hello')\n",
    "                row_values.append(df.iloc[idx, col_idx])\n",
    "                #print(f\"Row values for {col_idx}: {row_values}\")\n",
    "            # Debug: print row_values\n",
    "            #print(f\"Row values for row {idx}: {row_values}\")\n",
    "            \n",
    "            gathered_values.extend(row_values)  # Append row_values to gathered_values\n",
    "        #print(gathered_values)\n",
    "        return gathered_values\n",
    "    \n",
    "    \n",
    "    def gather_D(input_file, num_entries):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(input_file)\n",
    "        # Assuming row 3 is the row of interest (adjust index if necessary)\n",
    "        row_idx = 0  # Python uses zero-based indexing, so row 3 is index 2\n",
    "        # Find all column indices in row 3 where the value matches search_value\n",
    "        search_value = '#D'\n",
    "        # Check row 3 for the presence of search_value\n",
    "        row_3 = df.iloc[row_idx]\n",
    "        #print(row_3)\n",
    "        # Find all column indices in row 3 where the value matches search_value\n",
    "        column_indices = [idx for idx, value in enumerate(row_3) if value == search_value]\n",
    "        # Debug: print column_indices\n",
    "        print(f\"Column indices with '{search_value}' in row 3: {column_indices}\")\n",
    "        #print(col_idx)\n",
    "        # Initialize an empty list to store gathered values\n",
    "        gathered_values = []\n",
    "        # Iterate through each row starting from row 3\n",
    "        for idx in range(row_idx+1, len(df)):  # Start from row 4 onwards (zero-based index)\n",
    "            row_values = [0]  # Start each row \n",
    "            # Gather all values from identified columns\n",
    "            for col_idx in column_indices:\n",
    "                #print('hello')\n",
    "                row_values.append(df.iloc[idx, col_idx])\n",
    "                #print(f\"Row values for {col_idx}: {row_values}\")\n",
    "            # Debug: print row_values\n",
    "            #print(f\"Row values for row {idx}: {row_values}\")\n",
    "            gathered_values.extend(row_values)  # Append row_values to gathered_values\n",
    "        #print(gathered_values)\n",
    "        return gathered_values\n",
    "        \n",
    "    \n",
    "    def create_repeated_string_list(repeated_statevalues, string_to_repeat):\n",
    "        return [string_to_repeat] * len(repeated_statevalues)\n",
    "    \n",
    "    def count_empty_cells_in_column_a(input_file):\n",
    "        empty_cell_count = 1\n",
    "        with open(input_file, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            # Iterate over each row in the CSV\n",
    "            for row in reader:\n",
    "                if len(row) > 0 and row[0] == '':\n",
    "                    empty_cell_count += 1\n",
    "                elif len(row) > 0 and row[0] != '':\n",
    "                    break  # Stop counting once a non-empty cell is found\n",
    "        return empty_cell_count\n",
    "    \n",
    "    def count_empty_cells_between_first_two_filled_cells_first_row(input_file):\n",
    "        empty_cells_count = 0\n",
    "        found_first_filled = False\n",
    "        with open(input_file, 'r') as file:\n",
    "            csv_reader = csv.reader(file)\n",
    "            # Iterate over each row in the CSV\n",
    "            for row in csv_reader:\n",
    "                for cell_value in row:\n",
    "                    if cell_value != '':  # Non-empty cell found\n",
    "                        if not found_first_filled:\n",
    "                            found_first_filled = True\n",
    "                        else:\n",
    "                            # Second filled cell found, return the count and stop\n",
    "                            return empty_cells_count\n",
    "                    elif found_first_filled:\n",
    "                        # Count empty cells after the first filled cell\n",
    "                        empty_cells_count += 1\n",
    "        return empty_cells_count\n",
    "\n",
    "    def count_empty_cells_from_csv(input_file):\n",
    "        #first_value, second_value = find_first_two_filled_cells(input_file)\n",
    "        col_empty_cell_count = 0\n",
    "        with open(input_file, mode='r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "        #if first_value is not None and second_value is not None:\n",
    "        # Find indices of the first two filled cells in the first column\n",
    "        first_filled_index = None\n",
    "        second_filled_index = None\n",
    "        for row_idx, row in enumerate(data):\n",
    "            if len(row) > 0 and row[0] != \"\":\n",
    "                if first_filled_index is None:\n",
    "                    first_filled_index = row_idx\n",
    "                elif second_filled_index is None:\n",
    "                    second_filled_index = row_idx\n",
    "                    break  # Stop after finding the second filled cell\n",
    "        # Count empty cells between the first and second filled cells\n",
    "        for i in range(first_filled_index, second_filled_index):\n",
    "            if len(data[i]) > 0 and data[i][0] == \"\":\n",
    "                col_empty_cell_count += 1\n",
    "        #print(f\"Number of empty cells between {first_value} and {second_value}: {empty_cell_count}\")\n",
    "        return col_empty_cell_count\n",
    "\n",
    "    ################################################################################################\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    PDFgeneration = float(request.form.get('gen_pdf',0))\n",
    "    \n",
    "    #Determine File Type\n",
    "    file_typeDoH = float(request.form['file_type'])\n",
    "    if file_typeDoH == 1:\n",
    "        # Read CSV file if uploaded\n",
    "        input_file = request.files['csv_file']\n",
    "        upload_folder = os.path.join('/home', 'site', 'wwwroot', 'uploads')  # Azure's persistent storage location\n",
    "        os.makedirs(upload_folder, exist_ok=True)\n",
    "        input_file_path = os.path.join(upload_folder, input_file.filename)\n",
    "        input_file.save(input_file_path)  # Save the uploaded file\n",
    "        # Upload the file to Azure Blob Storage\n",
    "        #input_blob_name = input_file.filename\n",
    "        #input_file_bytes = BytesIO(input_file.read())\n",
    "        #upload_to_blob_storage(container_client, input_file_bytes, input_blob_name)\n",
    "        # Download the file from Azure Blob Storage\n",
    "        #input_file_data = download_blob_as_bytes(container_client, input_blob_name)\n",
    "        #input_filecon = input_file_data #.getvalue().decode('utf-8')\n",
    "        # Parse CSV content\n",
    "        #input_filecon = input_filecon1.splitlines()\n",
    "        input_csv_file = 'output_file.csv'\n",
    "        # Process the first line of the CSV file with specified encoding\n",
    "        try:\n",
    "            num_entries, cleaned_and_divided_entries = process_first_line(input_file_path, encoding='utf-8')\n",
    "            rows_after_second = count_rows_after_second(input_file_path, encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            print(\"UnicodeDecodeError encountered with 'utf-8' encoding, trying 'iso-8859-1' encoding\")\n",
    "            num_entries, cleaned_and_divided_entries = process_first_line(input_file_path, encoding='iso-8859-1')\n",
    "            rows_after_second = count_rows_after_second(input_file_path, encoding='iso-8859-1')\n",
    "        empty_cells_count = count_empty_cells_in_column_a(input_file_path)\n",
    "        print(f\"Number of empty cells in column A: {empty_cells_count}\")\n",
    "        result_empty_cells_count = count_empty_cells_between_first_two_filled_cells_first_row(input_file_path)\n",
    "        print(f\"Number of empty cells between the first two filled cells in the first row: {result_empty_cells_count}\")\n",
    "        col_empty_cell_count = count_empty_cells_from_csv(input_file_path)\n",
    "        print(f\"Number of empty cells between the first two filled cells in the first column: {col_empty_cell_count}\")\n",
    "        input_startvalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('Start')\n",
    "            GD_index = third_row.index('Start RT')\n",
    "            print(f\"RT index: {GD_index}\")\n",
    "            gather_D_index = third_row.index('#D')\n",
    "            print(f\"Gather D Index: {gather_D_index}\")\n",
    "            for row in reader:\n",
    "                if row: \n",
    "                    input_startvalues.append(row[column_index])\n",
    "                    input_startvalues = list(filter(None, input_startvalues))\n",
    "        repeated_startvalues = []\n",
    "        for value in input_startvalues:\n",
    "            repeated_startvalues.extend([value] * num_entries)\n",
    "        input_statevalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('State')\n",
    "            for row in reader:\n",
    "                if row:  \n",
    "                    input_statevalues.append(row[column_index])  \n",
    "                    input_statevalues = list(filter(None, input_statevalues))\n",
    "        repeated_statevalues = []\n",
    "        for value in input_statevalues:\n",
    "            repeated_statevalues.extend([value] * num_entries) \n",
    "        repeated_exposurevalues = []\n",
    "        for i in range(len(input_statevalues)):\n",
    "            repeated_exposurevalues.extend(cleaned_and_divided_entries)\n",
    "        input_endvalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('End')\n",
    "            for row in reader:\n",
    "                if row: \n",
    "                    input_endvalues.append(row[column_index])\n",
    "                    input_endvalues = list(filter(None, input_endvalues))\n",
    "        repeated_endvalues = []\n",
    "        for value in input_endvalues:\n",
    "            repeated_endvalues.extend([value] * num_entries)\n",
    "        input_seqvalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('Sequence')\n",
    "            for row in reader:\n",
    "                if row: \n",
    "                    input_seqvalues.append(row[column_index])\n",
    "                    input_seqvalues = list(filter(None, input_seqvalues))\n",
    "        repeated_seqvalues = []\n",
    "        for value in input_seqvalues:\n",
    "            repeated_seqvalues.extend([value] * num_entries)\n",
    "        input_MaxUpvalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('Max D')\n",
    "            for row in reader:\n",
    "                if row: \n",
    "                    input_MaxUpvalues.append(row[column_index])\n",
    "                    input_MaxUpvalues = list(filter(None, input_MaxUpvalues))\n",
    "        repeated_MaxUpvalues = []\n",
    "        for value in input_MaxUpvalues:\n",
    "            repeated_MaxUpvalues.extend([value] * num_entries)\n",
    "        input_chargevalues = []\n",
    "        with open(input_file_path, 'r') as file:\n",
    "            reader = csv.reader(file)\n",
    "            for _ in range(empty_cells_count):\n",
    "                next(reader)\n",
    "            third_row = next(reader)\n",
    "            column_index = third_row.index('Charge')\n",
    "            for row in reader:\n",
    "                if row: \n",
    "                    input_chargevalues.append(row[column_index])\n",
    "                    input_chargevalues = list(filter(None, input_chargevalues))\n",
    "        repeated_chargevalues = []\n",
    "        for value in input_chargevalues:\n",
    "            repeated_chargevalues.extend([value] * num_entries)\n",
    "        # Preset values for the first row in the output CSV\n",
    "        first_row_values = ['Protein', 'Start', 'End', 'Sequence', 'Modification',\n",
    "                           'Fragment', 'MaxUptake', 'MHP', 'State', 'Exposure',\n",
    "                           'File', 'z', 'RT', 'Inten', 'Center']\n",
    "        # Fill data_rows with rows of 15 blank values, based on the length of repeated_values\n",
    "        data_rows = [[''] * 15 for _ in range(len(repeated_statevalues))]\n",
    "        RTresult = gather_and_condense_columns(input_file_path, num_entries)\n",
    "        center = gather_D(input_file_path, num_entries)\n",
    "        #print(f\"Center: {center}\")\n",
    "        string_to_repeat = 'gen_prot' \n",
    "        repeated_protlist = create_repeated_string_list(repeated_statevalues, string_to_repeat)\n",
    "        string_to_repeat = 'gen_file'\n",
    "        repeated_filelist = create_repeated_string_list(repeated_statevalues, string_to_repeat)\n",
    "        string_to_repeat = '1'\n",
    "        repeated_charge = create_repeated_string_list(repeated_statevalues, string_to_repeat)\n",
    "        # Write to the output CSV file\n",
    "        #write_output_csv(output_file, first_row_values, data_rows, repeated_statevalues)\n",
    "        # Output the results\n",
    "        #print(f\"Number of empty cells in column A: {empty_cells_count}\")\n",
    "        print(f'Number of exposures: {num_entries}')\n",
    "        print(f'Exposure Times (converted to min): {cleaned_and_divided_entries}')\n",
    "        print(f'Number of rows after the second row (peptides): {rows_after_second}')\n",
    "        # Write to the output CSV file\n",
    "        csv_filename = \"output_file.csv\"\n",
    "        csv_path = os.path.join(os.getcwd(), csv_filename)\n",
    "        container_name = \"heatmap1vahidi\"\n",
    "        blob_client = blob_service_client.get_blob_client(container=container_name, blob=csv_filename)\n",
    "        write_output_csv(csv_path, first_row_values, data_rows, repeated_statevalues)\n",
    "        # Upload the output CSV to Azure Blob Storage\n",
    "        output_blob_name = generate_unique_filename('output_file', 'csv', timestamp)\n",
    "        #upload_to_blob_storage(container_client, csv_path, output_blob_name)\n",
    "        # Download the output CSV from Azure Blob Storage\n",
    "        input_csv_file = csv_path\n",
    "        #print(input_csv_file.decode())\n",
    "    else:\n",
    "        # Read CSV file if uploaded\n",
    "        input_csv_file = request.files['csv_file']\n",
    "    #allow push 333git#\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    #Col List\n",
    "    redc = '#c50f15'\n",
    "    dredc = '#990000'\n",
    "    bluec = '#0062cc'\n",
    "    dbluec = '#08306b'\n",
    "    whitec = '#FFFFFF'\n",
    "    blackc = '#000000'\n",
    "    greyc = '#E0E0E0'\n",
    "    greenc = '#006600'\n",
    "    yellowc = '#e69b00'\n",
    "    purplec = '#643d6e'\n",
    "\n",
    "    c1 = bluec #blue \n",
    "    c2 = whitec   #white\n",
    "    c3 = redc #red\n",
    "    c_missing = '#bdbdbd' # gray;\n",
    "    \n",
    "    #Gather user inputs\n",
    "\n",
    "    #For Woods Plots\n",
    "    color_by_heatmap = float(request.form.get('colorbyheatmap',0))\n",
    "    colcutopt = float(request.form.get('colcutopt',0))\n",
    "    if colcutopt == 1:\n",
    "        colcutoff = float(request.form.get('colcutoff',0.5))\n",
    "    else:\n",
    "        colcutoff = 0.5\n",
    "    woodsdimen = float(request.form.get('woodsdimen',0))\n",
    "    if woodsdimen == 1:\n",
    "        woodsx = float(request.form.get('woodsx',20))\n",
    "        woodsy = float(request.form.get('woodsy',6))\n",
    "    else:\n",
    "        woodsx = 20\n",
    "        woodsy = 6\n",
    "    woodscol = float(request.form.get('woodscol',0))\n",
    "    if woodscol == 1:\n",
    "        woodscolpos = request.form.get('woodscolpos','#FF0000')\n",
    "        woodscolneu = request.form.get('woodscolneu',whitec)\n",
    "        woodscolneg = request.form.get('woodscolneg','#0000FF')\n",
    "    else:\n",
    "        woodscolpos = '#FF0000'\n",
    "        woodscolneu = whitec\n",
    "        woodscolneg = '#0000FF'\n",
    "    nolines = float(request.form.get('nolines', 0))\n",
    "    \n",
    "    #Set font sizes\n",
    "    font_size = float(request.form['font_size'])\n",
    "    font_size_title = float(request.form['font_size_title'])\n",
    "    fontcolor = blackc\n",
    "    fontcolorc = float(request.form.get('fontcolor',2))\n",
    "    if fontcolorc == 1: #White\n",
    "        fontcolor = whitec \n",
    "    elif fontcolorc == 2: #Black\n",
    "        fontcolor = blackc\n",
    "    elif fontcolorc == 3: #Blue\n",
    "        fontcolor = dbluec\n",
    "    elif fontcolorc == 4: #Red\n",
    "        fontcolor = dredc\n",
    "    elif fontcolorc == 5: #Grey\n",
    "        fontcolor = greyc\n",
    "        \n",
    "    #Set horizontal or vertical plots\n",
    "    h_or_v = float(request.form['h_or_v'])\n",
    "    if h_or_v == 1:\n",
    "        plot_v = 0\n",
    "        plot_h = 1\n",
    "        plot_w = 0\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 2:\n",
    "        plot_v = 1\n",
    "        plot_h = 0\n",
    "        plot_w = 0\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 3:\n",
    "        plot_v = 0\n",
    "        plot_h = 0\n",
    "        plot_w = 1\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 4:\n",
    "        plot_v = 0\n",
    "        plot_h = 0\n",
    "        plot_w = 0\n",
    "        plot_volc = 1\n",
    "    if plot_volc == 1:\n",
    "        scatter_plot = 1\n",
    "    elif plot_volc == 0:\n",
    "        scatter_plot = 2020\n",
    "        \n",
    "    zerobound = float(request.form.get('zerobound', 0))\n",
    "    \n",
    "    p_threshold = request.form.get('pthresh', 0.05)\n",
    "    if p_threshold == '':\n",
    "        p_threshold = 0.05\n",
    "    if p_threshold != 0.05 and p_threshold != '':\n",
    "        p_threshold = float(p_threshold)\n",
    "    #Choose between setting MaxRange/#Shades and CustomColours/Bounds\n",
    "    custb = float(request.form['option'])\n",
    "    if custb == 3:\n",
    "        max_range = float(request.form['max_range'])\n",
    "        max_range = round(max_range, 1)\n",
    "        num_shades = float(request.form['num_shades'])\n",
    "        num_shades = round(num_shades)\n",
    "        alt_col = float(request.form.get('alt_col', 0))\n",
    "        if alt_col == 1:\n",
    "            if max_range <= 2:\n",
    "                c1 = request.form.get('negcolalt',bluec).strip()\n",
    "                c3 = request.form.get('poscolalt',redc).strip()\n",
    "            else:\n",
    "                c1 = request.form.get('negcolalt',dbluec).strip()\n",
    "                c3 = request.form.get('poscolalt',dredc).strip()\n",
    "    elif custb == 2:\n",
    "        num_shades = 7\n",
    "        alt_col = float(request.form.get('alt_col', 0))\n",
    "        if alt_col == 0:\n",
    "            custom_colors = ['#023858', '#045A8D', '#0570B0', '#3690C0', '#74A9CF', '#A6BDDB', '#FFFFFF', '#FC9272', '#FB6A4A', '#EF3B2C', '#CB181D', '#A50F15', '#67000D']\n",
    "    elif custb == 1:\n",
    "        max_range = None\n",
    "        custom_bounds = []\n",
    "        numinputs = 8 #how many to try and read\n",
    "        for i in range(1, numinputs + 1):\n",
    "            nb = request.form.get(f'inputbn{i}', 2020).strip()\n",
    "            pb = request.form.get(f'inputbp{i}', 2020).strip()\n",
    "            if nb == 2020 or nb == '':\n",
    "                break\n",
    "            nb = float(nb)\n",
    "            pb = float(pb)\n",
    "            custom_bounds.extend([nb, pb])\n",
    "            if zerobound == 1:\n",
    "                custom_bounds.append(0)\n",
    "        numinputs = len(custom_bounds) // 2\n",
    "        if zerobound == 1:\n",
    "            numinputs == numinputs + 1\n",
    "        #Gather Colour Choices \n",
    "        neg_col = float(request.form['neg_col'])\n",
    "        pos_col = float(request.form['pos_col'])\n",
    "        num_col = 2*numinputs - 1\n",
    "        numbshades = numinputs-1\n",
    "        #Define negative colours - dark, medium, and light\n",
    "        if neg_col == 1: #red\n",
    "            if max(abs(x) for x in custom_bounds) >= 2:\n",
    "                dcoln = dredc\n",
    "            else:\n",
    "                dcoln = redc\n",
    "        elif neg_col == 2: #blue\n",
    "            if max(abs(x) for x in custom_bounds) >= 2:\n",
    "                dcoln = dbluec\n",
    "            else: \n",
    "                dcoln = bluec\n",
    "        elif neg_col == 3: #green\n",
    "            dcoln = greenc\n",
    "        elif neg_col == 4: #yellow\n",
    "            dcoln = yellowc\n",
    "        elif neg_col == 5: #purple\n",
    "            dcoln = purplec\n",
    "        elif neg_col == 6: #black\n",
    "            dcoln = blackc\n",
    "        #Define positive colours - dark, medium, and light\n",
    "        if pos_col == 1: #blue\n",
    "            if max(abs(x) for x in custom_bounds) >= 2:\n",
    "                dcolp = dbluec\n",
    "            else:\n",
    "                dcolp = bluec\n",
    "        elif pos_col == 2: #red\n",
    "            if max(abs(x) for x in custom_bounds) >= 2:\n",
    "                dcolp = dredc\n",
    "            else:\n",
    "                dcolp = redc\n",
    "        elif pos_col == 3: #green\n",
    "            dcolp = greenc\n",
    "        elif pos_col == 4: #yellow\n",
    "            dcolp = yellowc\n",
    "        elif pos_col == 5: #purple\n",
    "            dcolp = purplec\n",
    "        elif pos_col == 6: #black\n",
    "            dcolp = blackc\n",
    "        c1 = dcoln\n",
    "        c2 = whitec\n",
    "        c3 = dcolp\n",
    "        col_mid = [mc.to_hex(c2)]\n",
    "        cols1 = [colorFader(c1,c2,x/numbshades) for x in range(numbshades+1)]\n",
    "        cols2 = [colorFader(c2,c3,x/numbshades) for x in range(numbshades+1)]\n",
    "        if zerobound == 1:\n",
    "            custom_colors = cols1[:-1] +cols2[1:]\n",
    "        if zerobound == 0:\n",
    "            custom_colors = cols1[:-1] + col_mid + cols2[1:]\n",
    "    elif custb == 4:\n",
    "        max_range = None            \n",
    "        custom_bounds = []\n",
    "        numinputs = 8 #how many to try and read\n",
    "        for i in range(1, numinputs + 1):\n",
    "            nb = request.form.get(f'inputbn{i}4', 2020).strip()\n",
    "            pb = request.form.get(f'inputbp{i}4', 2020).strip()\n",
    "            if nb == 2020 or nb == '':\n",
    "                break\n",
    "            nb = float(nb)\n",
    "            pb = float(pb)\n",
    "            custom_bounds.extend([nb, pb])\n",
    "            if zerobound == 1:\n",
    "                custom_bounds.append(0)\n",
    "        numinputs = len(custom_bounds) // 2\n",
    "        if zerobound == 1:\n",
    "            numinputs == numinputs + 1\n",
    "        numbshades = numinputs-1\n",
    "        colorchoicetype = float(request.form['optionc'])\n",
    "        if colorchoicetype == 1:\n",
    "            negshadein = str(request.form['negcolorid']).strip()\n",
    "            posshadein = str(request.form['poscolorid']).strip()\n",
    "            c1 = negshadein\n",
    "            c2 = whitec\n",
    "            c3 = posshadein\n",
    "            col_mid = [mc.to_hex(c2)]\n",
    "            if zerobound == 1:\n",
    "                cols1 = [colorFader(c1,c2,x/numbshades) for x in range(numbshades)]\n",
    "                cols2 = [colorFader(c2,c3,x/numbshades) for x in range(numbshades)]\n",
    "                custom_colors = cols1[:-1] +cols2[1:]\n",
    "            if zerobound == 0:\n",
    "                cols1 = [colorFader(c1,c2,x/numbshades) for x in range(numbshades+1)]\n",
    "                cols2 = [colorFader(c2,c3,x/numbshades) for x in range(numbshades+1)]\n",
    "                custom_colors = cols1[:-1] + col_mid + cols2[1:]\n",
    "        elif colorchoicetype ==2: \n",
    "            custom_colors = []\n",
    "            nc_list = []\n",
    "            pc_list = []\n",
    "            col_mid = mc.to_rgb(whitec)  # assuming mc is matplotlib.colors\n",
    "            for i in range(1, numinputs):\n",
    "                nc = request.form.get(f'ninputcol{i}', 2020).strip()\n",
    "                pc = request.form.get(f'pinputcol{i}', 2020).strip()\n",
    "                if nc == 2020 or nc == '':\n",
    "                    break\n",
    "                nc_list.append(mc.to_rgb(nc))\n",
    "                pc_list.append(mc.to_rgb(pc))\n",
    "            for nc in nc_list:\n",
    "                custom_colors.append(nc)\n",
    "            if zerobound == 0:\n",
    "                custom_colors.append(col_mid)\n",
    "            for pc in pc_list:\n",
    "                custom_colors.append(pc)\n",
    "\n",
    "    #Determine if Advanced Options were chosen, if yes import those values\n",
    "    dtime = int(request.form.get('droptime', 0))\n",
    "    dpept = int(request.form.get('droppept', 0))\n",
    "    dprot = int(request.form.get('dropprot', 0))\n",
    "    renumdict = int(request.form.get('renumdict', 0))\n",
    "    iddict = int(request.form.get('mutiddict', 0))\n",
    "    mutdict = int(request.form.get('mutdict', 0))\n",
    "    slist = int(request.form.get('optionst', 1))\n",
    "    if renumdict == 1:\n",
    "        key1 = str(request.form['key1']).strip()\n",
    "        value1 = (request.form['value1'])\n",
    "        key2 = str(request.form['key2']).strip()\n",
    "        value2 = (request.form['value2'])\n",
    "        # Constructing the variable, currently can do 2 proteins, should improve\n",
    "        if key2 == '':\n",
    "            renumbering_dict = {key1: value1}\n",
    "        else:\n",
    "            renumbering_dict = {key1: value1, key2: value2}\n",
    "        print('renumbering_dict')\n",
    "        print(renumbering_dict)\n",
    "    if iddict == 1:\n",
    "        mutidWT = str(request.form['wtpro1']).strip()\n",
    "        mutidMUT = str(request.form['mutpro1']).strip()\n",
    "        mut_id_dict = {mutidMUT : mutidWT} \n",
    "    if mutdict == 1:\n",
    "        mutdictres = float(request.form['mutdictres'])\n",
    "        mutdictwt = request.form['mutdictwt'].strip()\n",
    "        mutation_dict = {mutdictres : mutdictwt}\n",
    "    if dtime == 1:\n",
    "        num_timedao = int(request.form.get('numtimedao', 7))\n",
    "        drop_times = []\n",
    "        for i in range(0, num_timedao+7):\n",
    "            value = request.form.get(f'dt{i}')\n",
    "            if value:\n",
    "                try:\n",
    "                    drop_times.append(float(value))\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        print('drop times list')\n",
    "        print(drop_times)\n",
    "    if dpept == 1:\n",
    "        drop_pept = []\n",
    "        num_dpept = int(request.form.get('numpeptd', 4))\n",
    "        for i in range(1, num_dpept+4):\n",
    "            dpeppro = request.form.get(f'dpeppro{i}', '')\n",
    "            dpepst = float(request.form.get(f'dpepst{i}', 0))\n",
    "            dpepend = float(request.form.get(f'dpepend{i}', 0))\n",
    "            if dpeppro:\n",
    "                drop_pept.append((dpeppro, dpepst, dpepend))\n",
    "    if dprot == 1:\n",
    "        drop_prot = []\n",
    "        dpro1 = request.form.get('dpro2', '')\n",
    "        dpro2 = request.form.get('dpro3', '')\n",
    "        dpro3 = request.form.get('dpro4', '')\n",
    "        dpro4 = request.form.get('dpro5', '')\n",
    "        dpro5 = request.form.get('dpro6', '')\n",
    "        dpro6 = request.form.get('dpro7', '')\n",
    "        drop_prot.append(dpro1)\n",
    "        drop_prot.append(dpro2)\n",
    "        drop_prot.append(dpro3)\n",
    "        drop_prot.append(dpro4)\n",
    "        drop_prot.append(dpro5)\n",
    "        drop_prot.append(dpro6)\n",
    "        drop_prot = [item for item in drop_prot if item] #strip empty\n",
    "        print('drop protein list')\n",
    "        print(drop_prot)\n",
    "    if slist == 2:\n",
    "        state1_list = []\n",
    "        state2_list = []\n",
    "        for i in range(0, 10):\n",
    "            s1 = request.form.get(f's1{i}', '')\n",
    "            s2 = request.form.get(f's2{i}', '')\n",
    "            # Only append non-empty strings to state1_list\n",
    "            if s1 != '':\n",
    "                state1_list.append(s1)\n",
    "                # Assign s2 a default value only if it's empty\n",
    "                if s2 == '':\n",
    "                    s2 = request.form.get('s21', '')\n",
    "            # Only append non-empty strings to state2_list\n",
    "            if s2 != '':\n",
    "                state2_list.append(s2)\n",
    "        # Ensure that state1_list and state2_list have the same length\n",
    "        state1_list = state1_list[:len(state2_list)]\n",
    "\n",
    "    separate_plots_pls = float(request.form.get('separate_plots','0'))\n",
    "    if separate_plots_pls == 1:\n",
    "        plot_separate = 1\n",
    "        plot_stacked = 0\n",
    "        print('separate')\n",
    "    else:    \n",
    "        plot_stacked  = 1\n",
    "        plot_separate = 0\n",
    "        print('not separate')\n",
    "    \n",
    "    buffer = BytesIO()  # saves plot to BytesIO buffer\n",
    "    \n",
    "    output_csv_file  = r'HDX processed data.csv'\n",
    "    output_pdf_file  = r\"uploads/HDX heatmap.pdf\"\n",
    "    \n",
    "    # File name, format, and resolution for a bitmap image output of heatmap plots.\n",
    "    # By default the script saves all plots into one combined pdf and individual bitmap files. Set output_bitmap = 0 if saved bitmaps are not needed.\n",
    "    # Plot files have names in the following style: name_x_N.format\n",
    "    # where x is either 'h' or 'v' and N is an integer starting at 0 and automatically incremented for each plot.\n",
    "    # Format must be one of the formats that matplotlib.pyplot.savefig() understands, e.g. 'png', 'jpg', 'tiff';\n",
    "    # (format can also be a vector graphics format such as 'pdf' or 'svg')\n",
    "    \n",
    "    output_bitmap = 1\n",
    "    output_bitmap_name = 'HDX heatmap'\n",
    "    output_bitmap_dpi = 100\n",
    "    dif_dpi = float(request.form.get('dif_dpi', 0))\n",
    "    print(dif_dpi)\n",
    "    print('this is if difdpi is working')\n",
    "    if dif_dpi != 0:\n",
    "        output_bitmap_dpi = float(request.form.get('dpi_in'))\n",
    "    print(output_bitmap_dpi)\n",
    "    if PDFgeneration == 1:\n",
    "        output_bitmap_format = 'pdf'\n",
    "    else:\n",
    "        output_bitmap_format = 'png'\n",
    "    \n",
    "    # By default, when altering mutant peptide sequence and protein ID, script prints one-line message for each row processed.\n",
    "    # If this is unwanted, set mutation_msg=0\n",
    "    \n",
    "    mutation_msg = 0\n",
    "    \n",
    " \n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Optional parameters to filter only significant data based on p-values, and to produce p-value scatter plots.\n",
    "    \n",
    "    # The script attemps calculating means and standard deviations for each D exposure time for each state based on any number\n",
    "    # of replicates present in the data. It then computes D uptake differences between different protein states/conditions\n",
    "    # and calculates p-values for each difference. If there are no replicates in the data (i.e. only a single measurement at each time)\n",
    "    # p-values are undetermined. Data is not filtered if there are no replicates.\n",
    "    # If replicates are present at least for some of data, by default the script applies p-value filtration to the data before it is plotted\n",
    "    # in the heatmap (and before pymol macros are generated): all insignificant D uptake differences are set to zero.\n",
    "    # The user can change p-value threshold for filtering the data, variable 'p_threshold'\n",
    "    # The user can elect to avoid any p-value filtration by setting variable 'significant_only' to zero.\n",
    "    # The user may also elect to produce volcano plots of p-value vs. D uptake difference. By default the script does not produce these plots.\n",
    "    # Location for saving the volcano plots is set by variable 'scatter_dir', which be default is subdirectory 'scatter_plots'\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    # Optional variables to control splitting of heatmap plots into smaller segments.\n",
    "    # Due to the nature of HDX data, heatmap plots tend to be extremely elongated rectangles, summarizing a few HDX time points across many peptides.\n",
    "    # Frequently it is Aesthetically Pleasing™ to break down such elongated plots into a few fragments which are less elongated. This script can split the plot into\n",
    "    # pieces by 3 criteria: by protein; into a defined number of pieces of same size; into an arbitrary number of pieces of arbitrary size, set by the user.\n",
    "    \n",
    "    # Splitting by protein is done by setting variable split_outp_by_prot. By default it is set to an empty string, which means no splitting.\n",
    "    # If split_outp_by_prot = 'all' then data from each protein ID will be drawn in its own plot;\n",
    "    # If split_outp_by_prot is set to a list of protein IDs, then the script will draw data from these IDs into separate plots.\n",
    "    # If protein IDs in the split_outp_by_prot list are enclosed by parentheses to make a python tuple, data from these protein IDs is combined into a single plot.\n",
    "    # Also, if the split_outp_by_prot is set to a list, then any protein IDs in the data but not in the list will be omitted from plotting. \n",
    "    # This is similar to using drop_prot variable as explained above.\n",
    "    # Example. Let's say input data contains 4 protein IDs in the 'Protein' column of the input csv: A, B, C, D. Then:\n",
    "    # 1) split_outp_by_prot = '' (default) and drop_prot = [] (default) creates a single plot with all 4 protein IDs in that plot.\n",
    "    # 2) split_outp_by_prot = '' and drop_prot = ['B', 'D'] creates a single plot with data from proteins A and C in it.\n",
    "    # 3) split_outp_by_prot = 'all' creates 4 plots - one for each protein.\n",
    "    # 4) split_outp_by_prot = ['A', 'B', 'C', 'D']  - same as #3.\n",
    "    # 5) split_outp_by_prot = [('A', 'B'), 'C', 'D']  - creates 3 plots; 1 containing proteins A and B, next containing protein C, 3rd containing D.\n",
    "    # 6) split_outp_by_prot = ['A', 'C'] - creates 2 plots; 1 containing protein A data, the other containing protein C data. B and D are not plotted.\n",
    "    # 7) split_outp_by_prot = [('A', 'C')] - same shape of output as #2, data from A and C in a single plot. However, in some cases the colors might be different from the case where\n",
    "    # drop_prot = ['B', 'D'] is used, as explained in the drop_prot variable section.\n",
    "    # Protein IDs in the split_outp_by_prot must match those in the input data. Using an unrecognized protein ID will raise KeyError.\n",
    "    # Splitting into smaller segments for plotting is applied to the data after dropping (if any), thus the above caveat applies\n",
    "    # even when data corresponding to a protein ID was present in the input but was removed from a final data frame during processing\n",
    "    # by setting the drop_prot variable, or the script auto-dropped that protein ID because all of the D uptake differences computed for that ID are empty.\n",
    "    \n",
    "    # Variable split_outp_chunks, if set to a non-zero integer N, causes the the plot to be split into N segments of same size (within 1 peptide).\n",
    "    # Variable split_outp_by_row, if set to a list containing integers, causes the the plot to be split into segments at the row numbers indicated in the list.\n",
    "    # This is the most flexible splitting option and can be used to divide the plots into arbitrary segments.\n",
    "    # Example: split_outp_by_row = [60, 100] breaks output heatmaps into 3 pieces: first 60 peptides from the input (i.e. top 60 rows in the final processed dataframe) into one plot,\n",
    "    # then next 40 peptides into another plot, and all the remaining peptides into the 3rd plot.\n",
    "    \n",
    "    # Which option for heatmap splitting is applied is decided based on variable settings, in the following increasing priority:\n",
    "    # split_outp_by_prot = '' (or []), split_outp_chunks = 0, split_outp_by_row = [] - default values, no splitting, lowest priority\n",
    "    # split_outp_by_row = list of integers; to use this keep split_outp_by_prot = '' and split_outp_chunks = 0\n",
    "    # split_outp_chunks = integer; to use this keep split_outp_by_prot = '' \n",
    "    # split_outp_by_prot = list of protein IDs\n",
    "    # if split_outp_by_prot is set to a string which is not empty and not 'all', the script will complain and produce no output plots\n",
    "    # split_outp_by_prot = 'all' highest priority\n",
    "    \n",
    "    # Splitting of heatmap plots into pieces has no effect on pymol macro output.\n",
    "    \n",
    "    split_outp_by_prot = 'all'\n",
    "    # split_outp_by_prot = ['PSA_MYCTU', ('PSB_MYCTU', 'PSB_T1A')]\n",
    "    # split_outp_by_prot = ['PSB_MYCTU']\n",
    "    # split_outp_chunks = 2\n",
    "    # split_outp_by_row = [60, 100]\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Plot heatmaps of all states stacked into a single figure or each state into a separate figure, or both?\n",
    "    # Be default only separate plots are produced. If stacked plots are desired set plot_stacked = 1\n",
    "    # Set plot_separate = 0 to avoid plotting separate figures\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    #Plot here:\n",
    "    if custom_colors != None and custom_bounds != None:\n",
    "        custom_bounds = list(set(custom_bounds))\n",
    "        print(len(custom_colors))\n",
    "        print(custom_colors)\n",
    "        print(custom_bounds)\n",
    "        print(len(custom_bounds))\n",
    "        if len(custom_colors)>=1 and len(custom_bounds)>=1 and (len(custom_bounds) != len(custom_colors)+1):\n",
    "            print('It appears that both custom_colors and custom_bounds lists are used together, but there is mismatch between expected numbers of entries.')\n",
    "            print('custom_bounds list must have exactly one more entry than custom_colors. Aborting processing.')\n",
    "            raise StopExecution\n",
    "    if custom_colors != None and custom_bounds != None:\n",
    "        if len(custom_colors)==0 and len(custom_bounds)==1:\n",
    "            print('custom_bounds list must contain two or more numbers. Aborting processing.')\n",
    "            raise StopExecution\n",
    "    if custom_bounds != None:\n",
    "        custom_bounds.sort()\n",
    "    \n",
    "    if (not plot_h) and (not plot_v) and (not plot_w):\n",
    "        print('Both variables \\'plot_h\\' and \\'plot_v\\' are set to zero. No heatmap plots will be produced.\\n')\n",
    "    if (not plot_stacked) and (not plot_separate):\n",
    "        print('Both variables \\'plot_stacked\\' and \\'plot_separate\\' are set to zero. No heatmap plots will be produced.\\n')\n",
    "    \n",
    "    # Read the input and start processing:\n",
    "    data_df = pd.read_csv(input_csv_file)\n",
    "    \n",
    "    if 'z' not in data_df.columns:\n",
    "        print('No charge data (\\'z\\' column) in the input file. Assuming z=1 for all peptides.')\n",
    "        print('Your D uptake values will be severely underestimated if actually z>1.\\n')\n",
    "        data_df['z'] = 1\n",
    "    \n",
    "    # Renumber residues, if desired:\n",
    "    if renumbering_dict != None:\n",
    "        if len(renumbering_dict)>0:\n",
    "            print('Renumbering residues of protein IDs specified in renumbering_dict.\\n')\n",
    "            for prot, shift in renumbering_dict.items():\n",
    "                shift = pd.to_numeric(shift, errors='coerce')\n",
    "                data_df.loc[data_df['Protein'] == prot, ['Start', 'End']] -= shift\n",
    "                #data_df.loc[ data_df['Protein']==prot, ['Start','End'] ] = data_df.loc[ data_df['Protein']==prot, ['Start','End'] ] - shift\n",
    "        \n",
    "    # Mutate protein, if desired (i.e. change 'Protein' and 'Sequence' for peptides in proteins specified in mut_id_dict)\n",
    "    if mutation_dict != None and mut_id_dict != None:\n",
    "        if len(mutation_dict)>0 and len(mut_id_dict)==0:\n",
    "            print(\"A list of residue mutations is provided in mutation_dict, but dictionary mut_id_dict is empty, thus it is impossible to know which protein IDs should be altered. No mutation will be applied.\\n\")\n",
    "    \n",
    "    if mut_id_dict != None:    \n",
    "        if len(mut_id_dict)>0:\n",
    "            data_df['Original'] = 1\n",
    "            for mut_id, wt_id in mut_id_dict.items():\n",
    "                mut_indices = data_df.loc[ data_df['Protein']==mut_id ].index\n",
    "                print(\"Attempting to change data entries of mutant protein '%s' to have protein IDs and peptide sequences same as wild type protein '%s'.\" % (mut_id, wt_id))\n",
    "                if len(mutation_dict)>0:\n",
    "                    print(\"Using list of mutations provided by the user in mutation_dic.\\n\")\n",
    "                else:\n",
    "                    print(\"No mutation list is provided in mutation_dict. Will look up wt peptide sequences from non-mutant protein state(s).\\n\")\n",
    "                    mut2wt_df = data_df.loc[data_df['Protein']==wt_id , ['Start','End','Sequence']].groupby(['Start','End']).first()\n",
    "                for idx in mut_indices:\n",
    "                    mut_start = data_df.loc[idx , 'Start']\n",
    "                    mut_end   = data_df.loc[idx , 'End']\n",
    "                    mut_state = data_df.loc[idx , 'State']\n",
    "                    mut_seq   = data_df.loc[idx , 'Sequence']\n",
    "                    # Count number of rows containing same peptide with the same 'State' string from the wt protein, abort execution if it's more than zero\n",
    "                    seq_exist = data_df.loc[ (data_df['Protein']==wt_id) & (data_df['Start']==mut_start) & (data_df['End']==mut_end) & (data_df['State']==mut_state) & data_df['Original'] ].shape[0]\n",
    "                    if seq_exist:\n",
    "                        print(\"Input data table row %d contains an entry for a peptide from mutant protein which should be turned into a wt peptide/protein:\\n\" % idx)\n",
    "                        print(data_df.loc[idx].to_frame().T)\n",
    "                        print(\"\\nHowever, wt protein already contains that peptide with an identical 'State' label.\\nTurning mutant into a wt peptide for the same state would create erroneous data for this peptide/state combo.\\nLikely mutant protein data is labeled with a wrong 'State' in the input csv. Aborting processing.\\n\")\n",
    "                        raise StopExecution\n",
    "                    if len(mutation_dict)>0:\n",
    "                        for res, aa in mutation_dict.items():\n",
    "                            if mut_start<=res and mut_end>=res:\n",
    "                                if mut_seq[res-mut_start]==aa:\n",
    "                                    print(\"mutation_dict specifies that residue #%d should be turned into %s. However, it already is %s. Aborting processing.\\n\" % (res, aa, aa))\n",
    "                                    raise StopExecution\n",
    "                                wt_seq = mut_seq[:res-mut_start] + aa + mut_seq[res-mut_start+1:]\n",
    "                    else:\n",
    "                        # This used to run fine on an older version of Pandas:\n",
    "                        # wt_seq    = mut2wt_df.loc[ (mut_start, mut_end) ][0]\n",
    "                        # But now it produces a warning FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels\n",
    "                        # Apparently we are no longer allowed to access n-th element of series by doing 'series[n-1]', our coding experience has been much improved by forcing us to use 'series.iloc[n-1]'\n",
    "                        wt_seq    = mut2wt_df.loc[ (mut_start, mut_end) ].iloc[0]\n",
    "                    if mutation_msg:\n",
    "                        print(\"Input row %d, changing protein %s peptide %d-%d %s state %s to %s %s\" % (idx, mut_id, mut_start, mut_end, mut_seq, mut_state, wt_id, wt_seq))\n",
    "                    data_df.loc[idx , 'Protein'] = wt_id\n",
    "                    data_df.loc[idx , 'Sequence'] = wt_seq\n",
    "                    data_df.loc[idx , 'Original'] = 0\n",
    "                print('')\n",
    "    \n",
    "    # Start d_uptake calculations.\n",
    "    # Decharge:\n",
    "    if file_typeDoH != 1:\n",
    "        data_df['Center'] = (data_df['Center']-mp) * data_df['z']\n",
    "\n",
    "    print(data_df)\n",
    "    \n",
    "    # Compute average and std.dev. values for replicates at each time point. This includes replicates of no D exposure samples, if present in the data:\n",
    "    data_avg = data_df.groupby(['Protein', 'Start', 'End', 'Sequence', 'State', 'Exposure']).agg({'Center': ['mean', 'std','count']})\n",
    "\n",
    "    # Reshape the dataframe so that each column represents one D exposure time.\n",
    "    # Then subtract time=0 column for each state from time !=0 columns to get D uptake values at each time:\n",
    "    \n",
    "    data_avg = data_avg.unstack('Exposure')\n",
    "    data_avg.loc[:,('Center','mean',slice(None))] = data_avg.loc[:,('Center','mean',slice(None))].sub( data_avg['Center','mean',0], axis=0 )\n",
    "    data_avg.loc[:,('Center','std',slice(None))]  = data_avg.loc[:,('Center','std',slice(None))].add( data_avg['Center','std',0].fillna(0), axis=0 )\n",
    "    data_avg.columns=data_avg.columns.set_levels(['d_uptake'],level=0)\n",
    "    data_avg.columns=data_avg.columns.rename('Parameter', level=1)\n",
    "    \n",
    "    # Drop no longer needed columns with time=0:\n",
    "    data_avg = data_avg.drop(columns=0, level='Exposure')\n",
    "    \n",
    "    # We could keep working with the df in its current shape, but if we transform 'State' from row into column labels,\n",
    "    # it could be marginally easier to make correct .loc[] selection for subsequent subtractions.\n",
    "    # Merely applying .unstack('State') creates the order of column labels that is not aesthetically pleasing, thus let's stack\n",
    "    # 'Exposure' first, then unstack both 'State' and 'Exposure' in that order:\n",
    "    data_avg=data_avg.stack('Exposure', future_stack=True).unstack(level=['State','Exposure'])\n",
    "    \n",
    "  \n",
    "    \n",
    "    # Make a list of states for D uptake difference calculation:\n",
    "    all_states = data_avg.columns.get_level_values('State').unique()\n",
    "    if len(all_states) == 1:\n",
    "        print('There is only 1 state in the input csv file: \\'%s\\'' % all_states[0])\n",
    "        print('Need 2 or more states to calculate D uptake difference.\\n')\n",
    "        raise StopExecution\n",
    "    if state1_list != None and state2_list != None:\n",
    "        if len(state1_list) != len(state2_list):\n",
    "            print('state1_list and state2_list (the lists of protein states for difference calculation) have unequal lengths.')\n",
    "            print('If this is not intentional, there may be some D uptake difference calculations missing from the output.\\n')\n",
    "        state_list = list(zip(state1_list, state2_list))\n",
    "    \n",
    "    if not state_list:\n",
    "        state_list=list(itertools.combinations(all_states,2))\n",
    "        print('A list of protein states for D uptake difference calculation was not provided.')\n",
    "        print('There are total of %d states in the input csv file.' % len(all_states))\n",
    "        print('Will calculate all-against-all D uptake differeces, total of %d comparisons.\\n' % len(state_list))\n",
    "    \n",
    "    # Calculate D uptake differences between states. Append data to data_avg:\n",
    "    diffs = pd.DataFrame()\n",
    "    for two_states in state_list:\n",
    "        state1 = two_states[0]\n",
    "        state2 = two_states[1]\n",
    "        if state1 == state2:\n",
    "            print('Found a request to calculate difference between a state \\'%s\\' and itself.' % state1)\n",
    "            print('There may be a typo, or the orders of states might be mixed up, in the \\'state1_list\\' or \\'state2_list\\'.\\n')\n",
    "        mean = data_avg.loc[:,('d_uptake','mean',state1,slice(None))] - data_avg.loc[:,('d_uptake','mean',state2,slice(None))].values\n",
    "        std  = data_avg.loc[:,('d_uptake','std',state1,slice(None))]  + data_avg.loc[:,('d_uptake','std',state2,slice(None))].values\n",
    "        pval = ttest_ind_from_stats( data_avg.loc[:,('d_uptake','mean',state1,slice(None))], data_avg.loc[:,('d_uptake','std',state1,slice(None))], data_avg.loc[:,('d_uptake','count',state1,slice(None))], data_avg.loc[:,('d_uptake','mean',state2,slice(None))].values, data_avg.loc[:,('d_uptake','std',state2,slice(None))].values, data_avg.loc[:,('d_uptake','count',state2,slice(None))].values, equal_var=False)[1]\n",
    "        pval = pval.rename(columns={'count': 'pval'}, level='Parameter')\n",
    "        # Right now columns are labeled with state1 at 'State' level.\n",
    "        # We want names of both state1 and state2 in there\n",
    "        new_state = state1 + ' - ' + state2\n",
    "        d = {state1 : new_state}\n",
    "        mean = mean.rename(columns=d, level='State')\n",
    "        std  = std.rename(columns=d, level='State')\n",
    "        pval = pval.rename(columns=d, level='State')\n",
    "        diffs = pd.concat([diffs, mean, std, pval], axis=1)\n",
    "    \n",
    "    # In an older version of Pandas it used to be possible to run the next line inside the 'for' loop above and then concatenate result to 'data_avg' before going to the next iteration of 'for'.\n",
    "    # diffs.columns=diffs.columns.set_levels(['Delta_d_uptake'],level=0)\n",
    "    # The only thing this line does, it takes all existing labels at level 0 (all of them are 'd_uptake') and renames them to 'Delta_d_uptake'.\n",
    "    # At some point a new property was added to pandas multiindex, 'codes'. Which may be convenient in some cases. But now there is a requirement\n",
    "    # that levels list and codes list must be consistent. So that line of code works on a 1st pass of the loop, since at that time there is only\n",
    "    # 'd_uptake'. But on 2nd pass it crashes, b/c now you are trying to set_levels using a list of len=1, while the list of codes has len=2\n",
    "    # I have not tested, perhaps it still is possible to use it with the keyword verify_integrity=False.\n",
    "    # It also should be possible to use .set_codes() to accomplish same result, but seems like more work.\n",
    "    # My solution is to delay appending 'diffs' to 'data_avg' - I'm no longer doing that inside the 'for' loop.\n",
    "    # I'm doing it after all mean and std. dev. have been computed and collected into 'diffs', then rename\n",
    "    # 'd_uptake' to 'Delta_d_uptake' once, and concatenate it to 'data_avg'. Arguably, concatenating 'diffs' to 'data_avg' is unnecessary, since I extract that data into\n",
    "    # 'all_deltas' soon afterwards. But at one point I liked having all processed data in a single df.\n",
    "    \n",
    "    diffs.columns=diffs.columns.set_levels(['Delta_d_uptake'],level=0)\n",
    "    data_avg=pd.concat([data_avg, diffs], axis=1)\n",
    "    \n",
    "    # All calculations complete. Save the data to csv:\n",
    "    try:\n",
    "        data_avg.to_csv(output_csv_file)\n",
    "        print(\"Processed data was written into a csv file '%s'\\n\" % output_csv_file)\n",
    "    except IOError:\n",
    "        curr_time = time.localtime()\n",
    "        time_string = \"%s.%s.%s_%s.%s.%s\" % (curr_time[:6])\n",
    "        new_output_csv_file = output_csv_file + \"_\" + time_string + \"_.csv\"\n",
    "        data_avg.to_csv(new_output_csv_file)\n",
    "        print(\"Could not write csv output into file '%s'\" % output_csv_file)\n",
    "        print(\"On Windows this often happens when file already exists and is open.\")\n",
    "        print(\"Your csv output was written into a file with a new file name containing an appended unique date and time string.\")\n",
    "        print(\"Date and time string is in the format of: year.month.day_hour.minute.second\")\n",
    "        print(\"New file name is '%s'\\n\" % new_output_csv_file)\n",
    "\n",
    "    # Drop time points unwanted by the user, individual peptides unwanted by the user, and entire proteins unwanted by the user:\n",
    "    if drop_times != None:\n",
    "        data_avg = data_avg.drop(columns=drop_times, level='Exposure')\n",
    "    if drop_pept != None:\n",
    "        data_avg = data_avg.drop(index=drop_pept)\n",
    "    if drop_prot != None:    \n",
    "        data_avg = data_avg.drop(index=drop_prot)\n",
    "    \n",
    "    # Start plotting by grabbing Delta_d_uptake from data_avg and finding the maximal difference\n",
    "    # in D uptake in the entire data set. This maximum will define the color range,\n",
    "    # unless user specified otherwise by setting the max_range variable or custom_bounds list:\n",
    "    all_deltas = data_avg.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "    max_delta_pos = all_deltas.max().max()  # axis=None does not work in older versions of pandas\n",
    "    max_delta_neg = all_deltas.min().min()\n",
    "    max_delta = max([abs(max_delta_pos), abs(max_delta_neg)])   \n",
    "    if custb == 2:\n",
    "        max_range = round(max_delta, 1)\n",
    "        if max_range <= 3:\n",
    "            max_range = 3\n",
    "    if max_range == 0:\n",
    "        max_range = 1\n",
    "    if custb == 2:\n",
    "        if alt_col == 1:\n",
    "            c1 = request.form.get('negcolalt',bluec)\n",
    "            c3 = request.form.get('poscolalt',redc)\n",
    "    \n",
    "    # A function to create bounds list for heatmap color bins. Output will be passed into matplotlib.colors.BoundaryNorm()\n",
    "    def make_bounds(tot_colors, max_delta=max_delta):\n",
    "        custb = float(request.form['option'])\n",
    "        zerobound = float(request.form.get('zerobound', 0))\n",
    "        if custb ==3:\n",
    "            max_range = float(request.form['max_range'])\n",
    "            max_range = round(max_range, 1)\n",
    "        if custb ==2:\n",
    "            max_range = round(max_delta, 1)\n",
    "        num_shades = np.ceil(tot_colors / 2).astype(int)   # larger by 1 for odd tot_colors  \n",
    "        num_shades = round(num_shades)\n",
    "        if max_range:\n",
    "            incr = (max_range - 0.5) / num_shades\n",
    "        else:\n",
    "            incr = np.ceil(10 * max_delta / num_shades)/10\n",
    "            max_range = incr * num_shades\n",
    "        if tot_colors % 2 == 0:\n",
    "            bounds = np.linspace(-max_range, max_range, num = tot_colors+1)\n",
    "            if zerobound == 1:\n",
    "                bounds = np.linspace(-max_range, max_range, num = tot_colors)\n",
    "                bounds = np.append(bounds, 0)\n",
    "        else:\n",
    "            if custb == 2 and alt_col == 0:\n",
    "                half = np.linspace(0.5, max_range, num = num_shades)\n",
    "            else:    \n",
    "                half = np.linspace(0.5, max_range, num = num_shades-1)\n",
    "            if zerobound == 0:\n",
    "                bounds = np.append(-half, half)\n",
    "            elif zerobound == 1:\n",
    "                bounds = np.append(-half, half)\n",
    "                bounds = np.append(bounds, 0)\n",
    "            bounds = np.sort(bounds)\n",
    "        if zerobound == 1:\n",
    "            bounds = np.append(bounds, 0)\n",
    "            bounds = list(set(bounds))\n",
    "            bounds = np.sort(bounds)\n",
    "        return bounds\n",
    "\n",
    "    print(\"Maximal difference in D uptake between protein states in the data is %.2f.\" % max_delta)\n",
    "    if max_range != None:\n",
    "        if max_range:\n",
    "            print(\"The user has set max_range=%.2f; using this value for the color scale instead of above maximal difference.\" % max_range)\n",
    "    print('')\n",
    "    \n",
    "    # Drop columns and rows that are completely empty in all_deltas:\n",
    "    old_shape = all_deltas.shape\n",
    "    all_deltas = all_deltas.dropna(axis=0, how='all') # rows\n",
    "    all_deltas = all_deltas.dropna(axis=1, how='all') # columns\n",
    "    if all_deltas.shape[0] < old_shape[0]:\n",
    "        print('Before plotting, removed %d rows from D uptake difference table that are completely empty. This happens when\\nsome peptides are present in only one of the protein states.\\nYou may wish to inspect output csv file to make sure processed data looks reasonable.\\n' % (old_shape[0] - all_deltas.shape[0]))\n",
    "    if all_deltas.shape[1] < old_shape[1]:\n",
    "        print('Before plotting, removed %d columns from D uptake difference table that are completely empty. This happens when\\nsome time points are present in only one of the protein states.\\nYou may wish to inspect output csv file to make sure processed data looks reasonable.\\n' % (old_shape[1] - all_deltas.shape[1]))\n",
    "    all_pvals           = data_avg.loc[:,('Delta_d_uptake','pval',slice(None),slice(None))]        # after per forming .dropna() on all_deltas, potentially there are more columns in all_pvals than in all_deltas\n",
    "    columns_of_interest = all_deltas.rename(columns={'mean': 'pval'}, level='Parameter').columns   # columns from all_deltas, renamed to look like from all_pvals\n",
    "    all_pvals           = all_pvals.loc[:, columns_of_interest]                                    # now same columns are in all_pvals as in all_deltas\n",
    "    all_pvals           = all_pvals.loc[all_deltas.index, :]                                       # same for rows\n",
    "    \n",
    "    # Make p-value vs. D uptake difference volcano plots: create a directory; then loop through all states and times to plot data\n",
    "    all_states = all_deltas.columns.get_level_values('State').unique()\n",
    "\n",
    "    print('prot list')\n",
    "    all_prot = all_deltas.groupby('Protein')\n",
    "    #print(all_prot)\n",
    "    numprotinfile = len(all_prot)\n",
    "    print(numprotinfile)\n",
    "\n",
    "    def is_valid_hex_color(value):\n",
    "        hex_pattern = re.compile(r'^#[0-9A-Fa-f]{6}$')\n",
    "        return bool(hex_pattern.match(value))\n",
    "\n",
    "    ######SCATTER PLOTTING#######\n",
    "    p_threshold = request.form.get('pthresh', 0.05)\n",
    "    altvolc = request.form.get('altvolc', 2020)\n",
    "    if altvolc == 1:\n",
    "        altvolccol = request.form.get('altvolccol', blackc)\n",
    "        if is_valid_hex_color(altvolccol) == True:\n",
    "            scattercolor = altvolccol\n",
    "        else:\n",
    "            scattercolor = blackc\n",
    "    else:\n",
    "        scattercolor = blackc\n",
    "    \n",
    "    if p_threshold == '':\n",
    "        p_threshold = 0.05\n",
    "    if p_threshold != 0.05 and p_threshold != '':\n",
    "        p_threshold = float(p_threshold)\n",
    "    if scatter_plot != 2020:\n",
    "        all_deltas_grouped = all_deltas.groupby('Protein')\n",
    "        if scatter_dir:\n",
    "            Path(scatter_dir).mkdir(parents=True, exist_ok=True)\n",
    "        scatter_buffer = io.BytesIO()  # Create an in-memory zip file buffer\n",
    "        with zipfile.ZipFile(scatter_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "            for prot, data_subset in all_deltas_grouped:\n",
    "                for state in all_states:\n",
    "                    all_times = all_deltas.loc[:, ('Delta_d_uptake', 'mean', state, slice(None))].columns.get_level_values('Exposure').unique()\n",
    "                    for i in all_times:\n",
    "                        title = f'{state}, exposure: {i}'\n",
    "                        fig = plt.figure(figsize=(12, 10))\n",
    "                        plt.plot(all_deltas.loc[:, ('Delta_d_uptake', 'mean', state, i)], -all_pvals.loc[:, ('Delta_d_uptake', 'pval', state, i)].map(np.log10), markerfacecolor=scattercolor, marker='o')\n",
    "                        plt.axhline(y=-np.log10(p_threshold), color='k', linewidth=1)\n",
    "                        plt.axvline(x=-Dd_threshold, color='k', linewidth=1)\n",
    "                        plt.axvline(x=Dd_threshold, color='k', linewidth=1)\n",
    "                        plt.title(title)\n",
    "                        plt.ylabel('-log$_{10}$(p-value)')\n",
    "                        plt.xlabel('Differince in Deuterium Uptake (Da)')\n",
    "                        plt.xlim([-max_delta * 1.05, max_delta * 1.05])\n",
    "                        file_name = f\"volcano_plot_{prot}_{state}_{str(i)}.{output_bitmap_format}\"\n",
    "                        plt.savefig(file_name, dpi=output_bitmap_dpi)\n",
    "                        plt.close()\n",
    "                        if scatter_dir:\n",
    "                            file_path = f\"{scatter_dir}/{file_name}\"\n",
    "                        else:\n",
    "                            file_path = file_name\n",
    "                        zip_file.write(file_name)  # Add the file to the zip archive\n",
    "                        Path(file_name).unlink()  # Remove the file after adding it to the zip archive\n",
    "        # Save the zip buffer to a file and then upload to Azure Blob Storage\n",
    "        scatter_buffer.seek(0)\n",
    "        blob_client = blob_service_client.get_blob_client(container=CONTAINER_NAME, blob='scatter_plots.zip')\n",
    "        # Upload the in-memory zip file buffer to Azure Blob Storage\n",
    "        blob_client.upload_blob(scatter_buffer.getvalue(), overwrite=True)\n",
    "    \n",
    "    # Data filtration based on p-values:\n",
    "    if significant_only:\n",
    "        count_pvals = all_pvals.notna().sum()\n",
    "        no_reps_bool = (count_pvals==0)\n",
    "        no_reps_tot  = no_reps_bool.sum()\n",
    "        all_pvals_bool = (all_pvals>p_threshold)\n",
    "        all_deltas = all_deltas.mask(all_pvals_bool.values, 0)\n",
    "        #if no_reps_tot == len(count_pvals):   # all data is missing replicates\n",
    "        #    print('None of %d protein states and exposure time combinations appear to contain replicated data. No P-value filtering will be applied.\\n' % no_reps_tot)\n",
    "        #else:\n",
    "        #    if no_reps_tot > 0:   # looks like some, but not all data has missing replicates\n",
    "        #        print('%d of %d protein states and exposure time combinations do not have replicated data. P-value filtering is not applied to this data.\\n' % no_reps_tot, len(count_pvals))\n",
    "        #    print('Set %d D uptake differences with larger than %.2e p-values to 0 before plotting the data.\\n' % (all_pvals_bool.sum().sum(), p_threshold))\n",
    "    \n",
    "    #Setup colormap color list and bin boundaries:\n",
    "    if custom_colors != None:\n",
    "        if custom_colors:\n",
    "            colors = custom_colors\n",
    "            if custom_bounds != None:\n",
    "                if custom_bounds:\n",
    "                    bounds = custom_bounds\n",
    "                    print('bounds and ccol')\n",
    "            else:\n",
    "                bounds = make_bounds(len(colors))\n",
    "                print('no bounds found')\n",
    "    elif custom_bounds != None:\n",
    "        if custom_bounds:\n",
    "            print(\"The user has provided custom_bounds list and no custom_colors list. Using custom_bounds to calculate colors, and ingnoring max_range and num_shades variables.\\n\")\n",
    "            b = np.array(custom_bounds)\n",
    "            # the 3 variables below assume there is a zero value in the custom_bounds list, which divides all data into positive colors and negative colors:\n",
    "            neg_shades = len(b[b<0])\n",
    "            pos_shades = len(b[b>0])\n",
    "            col_mid = []\n",
    "            # if zero is absent from the custom_bounds list, then set the middle color to c2 (default white), but only if custom_bounds list has at least one value on each side of zero!\n",
    "            # Also, decrease number of shades on each side of zero by one:\n",
    "            if len(b[b==0])==0:\n",
    "                if neg_shades>0 and pos_shades>0:\n",
    "                    col_mid = [ mc.to_hex(c2) ]\n",
    "                    if zerobound == 0:\n",
    "                        neg_shades = len(b[b<0])-1\n",
    "                        pos_shades = len(b[b>0])-1\n",
    "                    if zerobound == 1:\n",
    "                        neg_shades = len(b[b<0])\n",
    "                        pos_shades = len(b[b>0])\n",
    "            if neg_shades > 0:\n",
    "                cols1 = [colorFader(c1,c2,x/neg_shades) for x in range(neg_shades+1)]\n",
    "            else:\n",
    "                cols1 = [ mc.to_hex(c2) ]\n",
    "            if pos_shades>0:\n",
    "                cols2 = [colorFader(c2,c3,x/pos_shades) for x in range(pos_shades+1)]\n",
    "            else:\n",
    "                cols2 = [ mc.to_hex(c2) ]\n",
    "            if zerobound == 0:\n",
    "                colors = cols1[:-1] + col_mid + cols2[1:]\n",
    "            if zerobound == 1:\n",
    "                colors = cols1[:-1] + cols2[1:]\n",
    "            bounds = custom_bounds\n",
    "            print(bounds)\n",
    "            print('no ccol')\n",
    "    else:\n",
    "        # c_missing is color to use in the heatmap for the missing data in the input file.\n",
    "        # c_missing is used regardless whether the user provided a color map or not.\n",
    "        c_missing = '#bdbdbd' # gray;\n",
    "        num_shades = round(num_shades) # Convert num_shades to integer if it's a float\n",
    "        if zerobound == 0:\n",
    "            cols1 = [colorFader(c1,c2,x/num_shades) for x in range(num_shades+1)]\n",
    "            cols2 = [colorFader(c2,c3,x/num_shades) for x in range(num_shades+1)]\n",
    "            colors = cols1 + cols2[1:]\n",
    "        if zerobound == 1:\n",
    "            cols1 = [colorFader(c1, c2, (x/num_shades)) for x in range(num_shades+1)]\n",
    "            cols2 = [colorFader(c2, c3, (x/num_shades)) for x in range(num_shades+1)]\n",
    "            colors = cols1[:-1] + cols2[1:]\n",
    "        bounds = make_bounds(len(colors))\n",
    "        if zerobound == 1:\n",
    "            bounds = np.append(bounds, 0)\n",
    "            bounds = list(set(bounds))\n",
    "            bounds = np.sort(bounds)\n",
    "        print(bounds)\n",
    "        print('no ccol or bounds found')\n",
    "\n",
    "    colormap = mc.ListedColormap(colors)\n",
    "    print(colors)\n",
    "    print(bounds)\n",
    "    print('right before calling')\n",
    "    my_norm = mc.BoundaryNorm(bounds, ncolors=len(colors))\n",
    "\n",
    "    all_deltas.reset_index(level=['Protein','Start','End'],inplace=True)\n",
    "        \n",
    "    # Open pdf output file:\n",
    "    pdf=PdfPages(output_pdf_file)  # This used to throw IOError if file is write-protected, but no longer does so.\n",
    "    try:\n",
    "        pdf.attach_note('')        # To test if file is write-protected, try adding a comment (empty string).\n",
    "    except IOError:\n",
    "        curr_time = time.localtime()\n",
    "        time_string = \"%s.%s.%s_%s.%s.%s\" % (curr_time[:6])\n",
    "        new_output_pdf_file = output_pdf_file + \"_\" + time_string + \"_.pdf\"\n",
    "        pdf=PdfPages(new_output_pdf_file)\n",
    "        print(\"Could not write pdf output into file '%s'\" % output_pdf_file)\n",
    "        print(\"On Windows this often happens when file already exists and is open.\")\n",
    "        print(\"Your pdf output was written into a file with a new file name containing an appended unique date and time string.\")\n",
    "        print(\"Date and time string is in the format of: year.month.day_hour.minute.second\")\n",
    "        print(\"New file name is '%s'\\n\" % new_output_pdf_file)\n",
    "    plt.rcParams['font.size'] = font_size\n",
    "    plt.rcParams['text.color'] = fontcolor\n",
    "    plt.rcParams['axes.labelcolor'] = fontcolor\n",
    "    \n",
    "    what_t_unit = 0\n",
    "    all_in_min = float(request.form.get('all_in_min',0))\n",
    "    if all_in_min == 1:\n",
    "        what_t_unit = float(request.form.get('what_t_unit',0))\n",
    "        \n",
    "    def make_time_tick_labels(time_list):\n",
    "        labels = []\n",
    "        if all_in_min == 0 or what_t_unit == 0:\n",
    "            for hdx_time in time_list:\n",
    "                if hdx_time < 1:\n",
    "                    string = '%s s' % np.round(hdx_time*60).astype(int)\n",
    "                elif hdx_time < 60:\n",
    "                    mins = np.floor(hdx_time).astype(int)\n",
    "                    sec = np.round(np.remainder(hdx_time, 1)*60).astype(int)\n",
    "                    if sec > 0:\n",
    "                        string = '%s m %s s' % (mins, sec)\n",
    "                    else:\n",
    "                        string = '%s m' % mins\n",
    "                else:\n",
    "                    hour = np.round(hdx_time/60).astype(int)\n",
    "                    mins  = np.round(np.remainder(hdx_time, 60)).astype(int)\n",
    "                    if mins > 0:\n",
    "                        string = '%s h %s m' % (hour, mins)\n",
    "                    else:\n",
    "                        string = '%s h' % hour\n",
    "                labels.append(string)\n",
    "        else:\n",
    "            if what_t_unit == 1: #s\n",
    "                for hdx_time in time_list:\n",
    "                    secs = (np.round(hdx_time).astype(int))*60\n",
    "                    string = f'{secs} s'\n",
    "                    labels.append(string)\n",
    "            if what_t_unit == 2: #m\n",
    "                for hdx_time in time_list:\n",
    "                    if hdx_time >= 1:\n",
    "                        mins = np.round(hdx_time).astype(int)\n",
    "                        string = f'{mins} m'\n",
    "                    else:\n",
    "                        string = f'{hdx_time} m'.rstrip('0').rstrip('.')\n",
    "                    labels.append(string)\n",
    "            if what_t_unit == 3: #h\n",
    "                for hdx_time in time_list:\n",
    "                    if hdx_time >= 60:\n",
    "                        hours = (np.round(hdx_time).astype(int))/60\n",
    "                        string = f'{hours} h'.rstrip('0').rstrip('.')\n",
    "                    else:\n",
    "                        hours = (np.round(hdx_time).astype(int))/60\n",
    "                        string = f'{hours:.2f} h'.rstrip('0').rstrip('.')\n",
    "                    labels.append(string)\n",
    "        return(labels)\n",
    "\n",
    "    #Set Cell Separating Values if not changed\n",
    "    hmspacerthick = 4\n",
    "    hmlcolor = whitec\n",
    "    #Set State Separating Values if not changed\n",
    "    hmsepthick = 4\n",
    "    hmsepcolorc = blackc\n",
    "    #Set Border Values if not changed\n",
    "    hmbordthick = 10\n",
    "    hmbordcolorc = blackc\n",
    "    #Set Tick Values if not changed\n",
    "    hmtl = 25 \n",
    "    hmtw = 4 \n",
    "    hmtcolor = blackc\n",
    "    hmtcolor_labels = blackc\n",
    "    \n",
    "    usehmthick = float(request.form.get('usehmthick', 0))\n",
    "    usestatesep = float(request.form.get('stackedplotdivl',0))\n",
    "    usebordchange = float(request.form.get('hmplotbord',0))\n",
    "    usehmtickchange = float(request.form.get('changehmtick',0))\n",
    "    if usehmthick == 1:\n",
    "        hmspacerthick = float(request.form.get('hmthickness',4)) \n",
    "        hmcolordivide = float(request.form.get('hmcolor',1))\n",
    "        if hmcolordivide == 1: #White\n",
    "            hmlcolor = whitec \n",
    "        elif hmcolordivide == 2: #Black\n",
    "            hmlcolor = blackc\n",
    "        elif hmcolordivide == 3: #Blue\n",
    "            hmlcolor = dbluec\n",
    "        elif hmcolordivide == 4: #Red\n",
    "            hmlcolor = dredc\n",
    "        elif hmcolordivide == 5: #Grey\n",
    "            hmlcolor = greyc\n",
    "    if usestatesep == 1:\n",
    "        hmsepthick = float(request.form.get('hmsepthickness',4)) \n",
    "        hmsepcolor = float(request.form.get('hmsepcolor',2))\n",
    "        if hmsepcolor == 1: #White\n",
    "            hmsepcolorc = whitec \n",
    "        elif hmsepcolor == 2: #Black\n",
    "            hmsepcolorc = blackc\n",
    "        elif hmsepcolor == 3: #Blue\n",
    "            hmsepcolorc = dbluec\n",
    "        elif hmsepcolor == 4: #Red\n",
    "            hmsepcolorc = dredc\n",
    "        elif hmsepcolor == 5: #Grey\n",
    "            hmsepcolorc = greyc    \n",
    "    if usebordchange == 1:\n",
    "        hmbordthick = float(request.form.get('hmbordthickness',10))\n",
    "        hmbordcolor = float(request.form.get('hmbordcolor',2))\n",
    "        if hmbordcolor == 1: #White\n",
    "            hmbordcolorc = whitec \n",
    "        elif hmbordcolor == 2: #Black\n",
    "            hmbordcolorc = blackc\n",
    "        elif hmbordcolor == 3: #Blue\n",
    "            hmbordcolorc = dbluec\n",
    "        elif hmbordcolor == 4: #Red\n",
    "            hmbordcolorc = dredc\n",
    "        elif hmbordcolor == 5: #Grey\n",
    "            hmbordcolorc = greyc \n",
    "    if usehmtickchange == 1:\n",
    "        hmtw = float(request.form.get('hmtickwidth',4))\n",
    "        hmtl = float(request.form.get('hmticklength',25))\n",
    "        hmtickcolor = float(request.form.get('hmtickcolor',2))\n",
    "        hmtickcolorlabel = float(request.form.get('hmtickcolorlabel',2))\n",
    "        #Color Ticks\n",
    "        if hmtickcolor == 1: #White\n",
    "            hmtcolor = whitec\n",
    "        elif hmtickcolor == 2: #Black\n",
    "            hmtcolor = blackc\n",
    "        elif hmtickcolor == 3: #Blue\n",
    "            hmtcolor = dbluec\n",
    "        elif hmtickcolor == 4: #Red\n",
    "            hmtcolor = dredc\n",
    "        elif hmtickcolor == 5: #Grey\n",
    "            hmtcolor = greyc\n",
    "        #Color Labels\n",
    "        if hmtickcolorlabel == 1: #White\n",
    "            hmtcolor_labels = whitec \n",
    "        elif hmtickcolorlabel == 2: #Black\n",
    "            hmtcolor_labels = blackc\n",
    "        elif hmtickcolorlabel == 3: #Blue\n",
    "            hmtcolor_labels = dbluec\n",
    "        elif hmtickcolorlabel == 4: #Red\n",
    "            hmtcolor_labels = dredc\n",
    "        elif hmtickcolorlabel == 5: #Grey\n",
    "            hmtcolor_labels = greyc\n",
    "\n",
    "    padthick = 20 #padding for titles\n",
    "    spadthick = 10 #padding for axis labels\n",
    "    usealtpad = float(request.form.get('usealtpad',20))\n",
    "    if usealtpad == 1:\n",
    "        padthick = float(request.form.get('altpad',20))\n",
    "        spadthick = float(request.form.get('altpads',20))\n",
    "\n",
    "    #color_sections = [\n",
    "    #    (5, 10, 'red', 'Section 1'),\n",
    "    #    (15, 20, 'blue', 'Section 2'),\n",
    "    #    (25, 30, 'green', 'Section 3')\n",
    "    #]\n",
    "    color_sections = []\n",
    "    domainlabel = float(request.form.get('domainlabel',0))\n",
    "    if domainlabel == 1:\n",
    "        numdomain = int(request.form.get('numdomain', 4))\n",
    "        padthick = padthick + 40 + (font_size_title-32)*2\n",
    "        for i in range(1, numdomain+4):\n",
    "            domainName = request.form.get(f'domainName{i}', '')\n",
    "            dompepst = float(request.form.get(f'dompepst{i}', 0)) - 1\n",
    "            dompepend = float(request.form.get(f'dompepend{i}', 0))\n",
    "            domColour = request.form.get(f'domColour{i}', '#000000')\n",
    "            if domainName:\n",
    "                color_sections.append((dompepst, dompepend, domColour, domainName))\n",
    "    \n",
    "    blockpositioning = -2.55 + (-font_size+28)/15\n",
    "    blocktextpos = -0.4 - 0.5*(font_size_title-32)/font_size_title \n",
    "    blockcolthick = 0.25\n",
    "    \n",
    "    heatmap_buffer = io.BytesIO()  # Create an in-memory zip file buffer\n",
    "    # Iterate through the list of computed D uptake differences and plot them:\n",
    "    def h_plot(pdf, data, title, pept, color_sections):\n",
    "        global output_bitmap_h_count\n",
    "        if isinstance(data, np.ma.MaskedArray):\n",
    "            data = data.filled(0)\n",
    "        all_prot = all_deltas.groupby('Protein')\n",
    "        if len(all_prot) > 1 and PDFgeneration != 1 and plot_stacked == 1:\n",
    "            with zipfile.ZipFile(heatmap_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "                # Generate the heatmap for each protein\n",
    "                fig_margin_x_l = 3\n",
    "                fig_margin_x_r = 1\n",
    "                fig_margin_y_t = 4\n",
    "                fig_margin_y_b = 3\n",
    "                fig_length_x = data.shape[0]\n",
    "                fig_length_y = data.shape[1]\n",
    "                fig_x = fig_margin_x_l + fig_length_x + fig_margin_x_r\n",
    "                fig_y = fig_margin_y_b + fig_length_y + fig_margin_y_t\n",
    "                cbar_spacer = 0.5  # Gap between heatmap plot and colorbar\n",
    "                cbar_length = min(len(colors), fig_length_x)  # 10 or fig_length_x, whichever is smaller\n",
    "                # Make tick labels\n",
    "                pept['Start'] = pept['Start'].round(0).astype(int)\n",
    "                pept['End'] = pept['End'].round(0).astype(int)\n",
    "                pept_tick_labels = pept['Start'].map(str) + '-' + pept['End'].map(str)\n",
    "                time_tick_labels = make_time_tick_labels(data.columns.get_level_values('Exposure'))\n",
    "                fig = plt.figure(figsize=(fig_x, fig_y), dpi=output_bitmap_dpi)\n",
    "                ax = plt.axes((fig_margin_x_l/fig_x, fig_margin_y_b/fig_y, fig_length_x/fig_x, fig_length_y/fig_y))\n",
    "                ax_4_cbar = plt.axes(((fig_margin_x_l+(fig_length_x-cbar_length)/2)/fig_x, (fig_margin_y_b - cbar_spacer - cbar_length/len(colors))/fig_y, cbar_length/fig_x, cbar_length/len(colors)/fig_y))\n",
    "                sns.heatmap(data.transpose(), ax=ax, cmap=colormap, norm=my_norm, xticklabels=pept_tick_labels, yticklabels=time_tick_labels, linewidths=hmspacerthick, linecolor=hmlcolor, square=True, cbar_ax=ax_4_cbar, cbar_kws={\"orientation\": \"horizontal\", 'label': u'Δ Deuterium Uptake'})\n",
    "                plt.sca(ax)\n",
    "                ax.set_facecolor(c_missing)\n",
    "                plt.title(title, fontsize=font_size_title, color=fontcolor, pad=padthick)\n",
    "                plt.setp(ax.spines.values(), linewidth=hmsepthick, color=hmsepcolorc)\n",
    "                plt.ylabel('H/D exchange time', labelpad=spadthick)\n",
    "                plt.xlabel('Peptides', labelpad=spadthick)\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.tick_params(axis='x', labelrotation=90, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.tick_params(axis='y', labelrotation=0, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.xaxis.set_tick_params(labelcolor=hmtcolor_labels)  # X-axis tick label color\n",
    "                ax.yaxis.set_tick_params(labelcolor=hmtcolor_labels)  # Y-axis tick label color\n",
    "                # Placing black line around the plot:\n",
    "                ax.axhline(y=0, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axhline(y=fig_length_y, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axvline(x=0, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axvline(x=fig_length_x, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                # Add separator lines between states:\n",
    "                states = data.columns.get_level_values('State').unique()\n",
    "                sep_line_y = 0\n",
    "                for i in range(len(states)-1):\n",
    "                    exposures = data.loc[:,('Delta_d_uptake','mean',states[i],slice(None))].columns.get_level_values('Exposure')\n",
    "                    sep_line_y += len(exposures)\n",
    "                    ax.axhline(y=sep_line_y, color=hmsepcolorc, linewidth=hmsepthick)\n",
    "                # Add colored blocks above the peptide axis\n",
    "                for section in color_sections:\n",
    "                    start, end, color, text = section\n",
    "                    # Adjust y position of the rectangle and annotation to be above the top ticks\n",
    "                    rect_y = blockpositioning  # Position above the top of the heatmap\n",
    "                    rect = plt.Rectangle((start, rect_y), end-start, blockcolthick, color=color, clip_on=False)\n",
    "                    ax.add_patch(rect)\n",
    "                    text_y = blocktextpos + rect_y + blockcolthick / 2\n",
    "                    ax.annotate(text, xy=((start+end)/2, text_y), xytext=(0,0), textcoords='offset points',\n",
    "                                ha='center', va='center', fontsize=font_size_title, color=fontcolor, clip_on=False, annotation_clip=False)\n",
    "                    print('adding section')\n",
    "                pdf.savefig(bbox_inches='tight')  # Saves the current figure into a pdf page\n",
    "                if output_bitmap:\n",
    "                    if plot_separate == 1:\n",
    "                        output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                    else: \n",
    "                        output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                    plt.savefig(output_bitmap_file, dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "                    output_bitmap_h_count += 1\n",
    "                if plot_separate == 1:\n",
    "                    output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + state + '.' + output_bitmap_format\n",
    "                else: \n",
    "                    output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '.' + output_bitmap_format\n",
    "                plt.savefig(output_bitmap_file, format='png', dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                zip_file.write(output_bitmap_file)  # Add the file to the zip archive\n",
    "                # Remove the file after adding it to the zip archive\n",
    "                Path(output_bitmap_file).unlink()\n",
    "            with open('heatmaps.zip', 'wb') as f:\n",
    "                f.write(heatmap_buffer.getvalue())\n",
    "        else:\n",
    "            with zipfile.ZipFile(heatmap_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "                fig_margin_x_l = 3\n",
    "                fig_margin_x_r = 1\n",
    "                fig_margin_y_t = 4\n",
    "                fig_margin_y_b = 3\n",
    "                fig_length_x = data.shape[0]\n",
    "                fig_length_y = data.shape[1]\n",
    "                fig_x = fig_margin_x_l + fig_length_x + fig_margin_x_r\n",
    "                fig_y = fig_margin_y_b + fig_length_y + fig_margin_y_t\n",
    "                cbar_spacer = 0.5  # Gap between heatmap plot and colorbar\n",
    "                cbar_length = min(len(colors), fig_length_x)  # 10 or fig_length_x, whichever is smaller\n",
    "                # Make tick labels\n",
    "                pept['Start'] = pept['Start'].round(0).astype(int)\n",
    "                pept['End'] = pept['End'].round(0).astype(int)\n",
    "                pept_tick_labels = pept['Start'].map(str) + '-' + pept['End'].map(str)\n",
    "                time_tick_labels = make_time_tick_labels(data.columns.get_level_values('Exposure'))\n",
    "                fig = plt.figure(figsize=(fig_x, fig_y), dpi=output_bitmap_dpi)\n",
    "                ax = plt.axes((fig_margin_x_l/fig_x, fig_margin_y_b/fig_y, fig_length_x/fig_x, fig_length_y/fig_y))\n",
    "                ax_4_cbar = plt.axes(((fig_margin_x_l+(fig_length_x-cbar_length)/2)/fig_x, (fig_margin_y_b - cbar_spacer - cbar_length/len(colors))/fig_y, cbar_length/fig_x, cbar_length/len(colors)/fig_y))\n",
    "                sns.heatmap(data.transpose(), ax=ax, cmap=colormap, norm=my_norm, xticklabels=pept_tick_labels, yticklabels=time_tick_labels, linewidths=hmspacerthick, linecolor=hmlcolor, square=True, cbar_ax=ax_4_cbar, cbar_kws={\"orientation\": \"horizontal\", 'label': u'Δ Deuterium Uptake'})\n",
    "                plt.sca(ax)\n",
    "                ax.set_facecolor(c_missing)\n",
    "                plt.title(title, fontsize=font_size_title, color=fontcolor, pad=padthick)\n",
    "                plt.setp(ax.spines.values(), linewidth=hmsepthick, color=hmsepcolorc)\n",
    "                plt.ylabel('H/D exchange time', labelpad=spadthick)\n",
    "                plt.xlabel('Peptides', labelpad=spadthick)\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.tick_params(axis='x', labelrotation=90, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.tick_params(axis='y', labelrotation=0, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.xaxis.set_tick_params(labelcolor=hmtcolor_labels)  # X-axis tick label color\n",
    "                ax.yaxis.set_tick_params(labelcolor=hmtcolor_labels)  # Y-axis tick label color\n",
    "                # Placing black line around the plot:\n",
    "                ax.axhline(y=0, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axhline(y=fig_length_y, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axvline(x=0, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                ax.axvline(x=fig_length_x, color=hmbordcolorc, linewidth=hmbordthick)\n",
    "                # Add separator lines between states:\n",
    "                states = data.columns.get_level_values('State').unique()\n",
    "                sep_line_y = 0\n",
    "                for i in range(len(states)-1):\n",
    "                    exposures = data.loc[:,('Delta_d_uptake','mean',states[i],slice(None))].columns.get_level_values('Exposure')\n",
    "                    sep_line_y += len(exposures)\n",
    "                    ax.axhline(y=sep_line_y, color=hmsepcolorc, linewidth=hmsepthick)\n",
    "                # Add colored blocks above the peptide axis\n",
    "                for section in color_sections:\n",
    "                    start, end, color, text = section\n",
    "                    # Adjust y position of the rectangle and annotation to be above the top ticks\n",
    "                    rect_y = blockpositioning  # Position above the top of the heatmap\n",
    "                    rect = plt.Rectangle((start, rect_y), end-start, blockcolthick, color=color, clip_on=False)\n",
    "                    ax.add_patch(rect)\n",
    "                    text_y = blocktextpos + rect_y + blockcolthick / 2 \n",
    "                    ax.annotate(text, xy=((start+end)/2, text_y), xytext=(0,0), textcoords='offset points',\n",
    "                                ha='center', va='center', fontsize=font_size_title, color=fontcolor, clip_on=False, annotation_clip=False)\n",
    "                    print('adding section')\n",
    "                pdf.savefig(bbox_inches='tight')  # Saves the current figure into a pdf page\n",
    "                if output_bitmap:\n",
    "                    if plot_separate == 1:\n",
    "                        output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                    else: \n",
    "                        output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                    plt.savefig(output_bitmap_file, dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "                    output_bitmap_h_count += 1\n",
    "                if plot_separate == 1:\n",
    "                    output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                else: \n",
    "                    output_bitmap_file = output_bitmap_name + '_h_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                if PDFgeneration == 1:\n",
    "                    plt.savefig(output_bitmap_file, format='pdf', dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "                else:\n",
    "                    plt.savefig(output_bitmap_file, format='png', dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                zip_file.write(output_bitmap_file)  # Add the file to the zip archive\n",
    "            with open('heatmaps.zip', 'wb') as f:\n",
    "                f.write(heatmap_buffer.getvalue())\n",
    "\n",
    "    def v_plot(pdf, data, title, pept):\n",
    "        global output_bitmap_v_count\n",
    "        all_prot = all_deltas.groupby('Protein')\n",
    "        if isinstance(data, np.ma.MaskedArray):\n",
    "            data = data.filled(0)\n",
    "        if len(all_prot) > 1 and PDFgeneration != 1 and plot_stacked == 1:\n",
    "            with zipfile.ZipFile(heatmap_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "                fig_margin_x_l = 3\n",
    "                fig_margin_x_r = 3\n",
    "                fig_margin_y_t = 3\n",
    "                fig_margin_y_b = 1\n",
    "                fig_length_x = data.shape[1]\n",
    "                fig_length_y = data.shape[0]\n",
    "                fig_x = fig_margin_x_l + fig_length_x + fig_margin_x_r\n",
    "                fig_y = fig_margin_y_b + fig_length_y + fig_margin_y_t\n",
    "                cbar_spacer = 0.5  # Gap between heatmap plot and colorbar\n",
    "                cbar_length  = min(len(colors), fig_length_y)  # 10 or fig_length_x, whichever is smaller\n",
    "                # make tick labels:\n",
    "                pept['Start'] = pept['Start'].round(0).astype(int)\n",
    "                pept['End'] = pept['End'].round(0).astype(int)\n",
    "                pept_tick_labels = pept['Start'].map(str) + '-' + pept['End'].map(str)\n",
    "                time_tick_labels = make_time_tick_labels( data.columns.get_level_values('Exposure') )\n",
    "                fig = plt.figure(figsize=(fig_x , fig_y), dpi=output_bitmap_dpi)\n",
    "                ax = plt.axes((fig_margin_x_l/fig_x, fig_margin_y_b/fig_y, fig_length_x/fig_x, fig_length_y/fig_y ))\n",
    "                ax_4_cbar = plt.axes(((fig_margin_x_l+fig_length_x+cbar_spacer)/fig_x, (fig_margin_y_b + (fig_length_y-cbar_length)/2)/fig_y, cbar_length/len(colors)/fig_x, cbar_length/fig_y ))\n",
    "                sns.heatmap(data, ax=ax, cmap=colormap, norm=my_norm, xticklabels=time_tick_labels, yticklabels=pept_tick_labels, linewidths=hmspacerthick, linecolor=hmlcolor, square=True, cbar_ax=ax_4_cbar, cbar_kws={\"orientation\": \"vertical\", 'label': u'Δ Deuterium Uptake'})\n",
    "                plt.sca(ax)\n",
    "                ax.set_facecolor(c_missing)\n",
    "                plt.title(title,fontsize=font_size_title, color=fontcolor, pad=padthick)\n",
    "                plt.setp(ax.spines.values(), linewidth=hmsepthick, color=hmsepcolorc)\n",
    "                plt.xlabel('H/D exchange time', labelpad=spadthick)\n",
    "                plt.ylabel('Peptides', labelpad=spadthick)\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.tick_params(axis = 'x', labelrotation = 30, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.tick_params(axis = 'y', labelrotation = 0, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.xaxis.set_tick_params(labelcolor=hmtcolor_labels)  # X-axis tick label color\n",
    "                ax.yaxis.set_tick_params(labelcolor=hmtcolor_labels)  # Y-axis tick label color\n",
    "                #Placing black line around the plot:\n",
    "                ax.axhline(y=0, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axhline(y=fig_length_y, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axvline(x=0, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axvline(x=fig_length_x, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                # Add separator lines between states:\n",
    "                states = data.columns.get_level_values('State').unique()\n",
    "                sep_line_x = 0\n",
    "                for i in range(len(states)-1):\n",
    "                    exposures = data.loc[:,('Delta_d_uptake','mean',states[i],slice(None))].columns.get_level_values('Exposure')\n",
    "                    sep_line_x += len(exposures)\n",
    "                    ax.axvline(x=sep_line_x, color=hmsepcolorc, linewidth=hmsepthick)\n",
    "                #print(pept_tick_labels)\n",
    "                #print(time_tick_labels)\n",
    "                pdf.savefig()  # saves the current figure into a pdf page\n",
    "                if output_bitmap:\n",
    "                    if plot_separate == 1:\n",
    "                        output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                    else: \n",
    "                        output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                    plt.savefig(output_bitmap_file, dpi=output_bitmap_dpi)\n",
    "                    output_bitmap_h_count += 1\n",
    "                if plot_separate == 1:\n",
    "                    output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                else: \n",
    "                    output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                plt.savefig(output_bitmap_file, format='png', dpi=output_bitmap_dpi)\n",
    "                buffer.seek(0)    \n",
    "                plt.close()\n",
    "                zip_file.write(output_bitmap_file)  # Add the file to the zip archive\n",
    "                # Remove the file after adding it to the zip archive\n",
    "                Path(output_bitmap_file).unlink()\n",
    "                #print('s4')\n",
    "            # Save the zip buffer to a file or return it as a response in Flask, depending on your use case\n",
    "            with open('heatmaps.zip', 'wb') as f:\n",
    "                f.write(heatmap_buffer.getvalue())\n",
    "        else:\n",
    "            with zipfile.ZipFile(heatmap_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "                fig_margin_x_l = 3\n",
    "                fig_margin_x_r = 3\n",
    "                fig_margin_y_t = 3\n",
    "                fig_margin_y_b = 1\n",
    "                fig_length_x = data.shape[1]\n",
    "                fig_length_y = data.shape[0]\n",
    "                fig_x = fig_margin_x_l + fig_length_x + fig_margin_x_r\n",
    "                fig_y = fig_margin_y_b + fig_length_y + fig_margin_y_t\n",
    "                cbar_spacer = 0.5  # Gap between heatmap plot and colorbar\n",
    "                cbar_length  = min(len(colors), fig_length_y)  # 10 or fig_length_x, whichever is smaller\n",
    "                # make tick labels:\n",
    "                pept['Start'] = pept['Start'].round(0).astype(int)\n",
    "                pept['End'] = pept['End'].round(0).astype(int)\n",
    "                pept_tick_labels = pept['Start'].map(str) + '-' + pept['End'].map(str)\n",
    "                time_tick_labels = make_time_tick_labels( data.columns.get_level_values('Exposure') )\n",
    "                fig = plt.figure(figsize=(fig_x , fig_y), dpi=output_bitmap_dpi)\n",
    "                ax = plt.axes((fig_margin_x_l/fig_x, fig_margin_y_b/fig_y, fig_length_x/fig_x, fig_length_y/fig_y ))\n",
    "                ax_4_cbar = plt.axes(((fig_margin_x_l+fig_length_x+cbar_spacer)/fig_x, (fig_margin_y_b + (fig_length_y-cbar_length)/2)/fig_y, cbar_length/len(colors)/fig_x, cbar_length/fig_y ))\n",
    "                sns.heatmap(data, ax=ax, cmap=colormap, norm=my_norm, xticklabels=time_tick_labels, yticklabels=pept_tick_labels, linewidths=hmspacerthick, linecolor=hmlcolor, square=True, cbar_ax=ax_4_cbar, cbar_kws={\"orientation\": \"vertical\", 'label': u'Δ Deuterium Uptake'})\n",
    "                plt.sca(ax)\n",
    "                ax.set_facecolor(c_missing)\n",
    "                plt.title(title,fontsize=font_size_title, color=fontcolor, pad=padthick)\n",
    "                plt.setp(ax.spines.values(), linewidth=hmsepthick, color=hmsepcolorc)\n",
    "                plt.xlabel('H/D exchange time', labelpad=spadthick)\n",
    "                plt.ylabel('Peptides', labelpad=spadthick)\n",
    "                ax.xaxis.tick_top()\n",
    "                ax.xaxis.set_label_position('top')\n",
    "                ax.tick_params(axis = 'x', labelrotation = 30, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.tick_params(axis = 'y', labelrotation = 0, length = hmtl, width = hmtw, colors=hmtcolor)\n",
    "                ax.xaxis.set_tick_params(labelcolor=hmtcolor_labels)  # X-axis tick label color\n",
    "                ax.yaxis.set_tick_params(labelcolor=hmtcolor_labels)  # Y-axis tick label color\n",
    "                #Placing black line around the plot:\n",
    "                ax.axhline(y=0, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axhline(y=fig_length_y, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axvline(x=0, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                ax.axvline(x=fig_length_x, color=hmbordcolorc,linewidth=hmbordthick)\n",
    "                # Add separator lines between states:\n",
    "                states = data.columns.get_level_values('State').unique()\n",
    "                sep_line_x = 0\n",
    "                for i in range(len(states)-1):\n",
    "                    exposures = data.loc[:,('Delta_d_uptake','mean',states[i],slice(None))].columns.get_level_values('Exposure')\n",
    "                    sep_line_x += len(exposures)\n",
    "                    ax.axvline(x=sep_line_x, color=hmsepcolorc, linewidth=hmsepthick)\n",
    "                #print(pept_tick_labels)\n",
    "                #print(time_tick_labels)\n",
    "                pdf.savefig()  # saves the current figure into a pdf page\n",
    "                if output_bitmap:\n",
    "                    if plot_separate == 1:\n",
    "                        output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                    else: \n",
    "                        output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                    plt.savefig(output_bitmap_file, dpi=output_bitmap_dpi)\n",
    "                    output_bitmap_h_count += 1\n",
    "                if plot_separate == 1:\n",
    "                    output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '_' + state + '.' + output_bitmap_format\n",
    "                else: \n",
    "                    output_bitmap_file = output_bitmap_name + '_v_' + str(output_bitmap_h_count) + '_' + prot + '.' + output_bitmap_format\n",
    "                if PDFgeneration == 1:\n",
    "                    plt.savefig(output_bitmap_file, format='pdf', dpi=output_bitmap_dpi)\n",
    "                else:\n",
    "                    plt.savefig(output_bitmap_file, format='png', dpi=output_bitmap_dpi)\n",
    "                plt.close()\n",
    "                zip_file.write(output_bitmap_file)  # Add the file to the zip archive\n",
    "            with open('heatmaps.zip', 'wb') as f:\n",
    "                f.write(heatmap_buffer.getvalue())\n",
    "\n",
    "    woodscolthick = 3\n",
    "    woodsbackthick = 0.5\n",
    "    altwoodsthick = float(request.form.get('altwoodsthick',0))\n",
    "    if altwoodsthick == 1:\n",
    "        woodscolthick = float(request.form.get('woodscolthick',3))\n",
    "        woodsbackthick = float(request.form.get('woodsbackthick',0.5))\n",
    "\n",
    "    wpadthick = 10\n",
    "    if domainlabel == 1:\n",
    "        wpadthick = wpadthick + 35\n",
    "    blockposwoods = max_delta + max_delta/10 #for w_plot\n",
    "    blocktextposwoods = 0.3 #for w_plot\n",
    "    blockthickwoods = 0.2\n",
    "    \n",
    "    def w_plot(pdf, data, title, pept, state, data_avg, max_delta, colcutoff):\n",
    "        # Ensure data_avg is correctly defined and used\n",
    "        # Ensure global variables are properly managed\n",
    "        global output_bitmap_h_count\n",
    "        if isinstance(data, np.ma.MaskedArray):\n",
    "            data = data.filled(0)\n",
    "        #print('data avg')\n",
    "        #print(data_avg)  # Debugging print statement\n",
    "        unique_exposures = data.columns.get_level_values('Exposure').unique()\n",
    "        #print('unique exposures')\n",
    "        #print(unique_exposures)  # Debugging print statement\n",
    "        numplots = len(unique_exposures)\n",
    "        #print('data avg info')\n",
    "        #print(data_avg.info())  # Debugging print statement\n",
    "        #print('data columns')\n",
    "        #print(data.columns)\n",
    "        data_avgrot = data_avg.transpose()\n",
    "        #print('pept')\n",
    "        #print(pept)\n",
    "        first_start = None\n",
    "        last_end = None\n",
    "        for exposure in unique_exposures:  # Use unique_exposures instead of exposures\n",
    "            plt.figure(figsize=(woodsx, woodsy))\n",
    "            # Extract relevant subset of data for current state and exposure\n",
    "            subset = data.xs(('Delta_d_uptake', 'mean', state, exposure), level=[None, 'Parameter', 'State', 'Exposure'], axis=1)  \n",
    "            if colcutopt == 0:\n",
    "                colcutoff = 0.5\n",
    "            if nolines == 0:\n",
    "                plt.axhline(y = 0, color = blackc) #0 line\n",
    "                plt.axhline(y = colcutoff, color = blackc, linestyle = 'dashed') #Cutoff lines \n",
    "                plt.axhline(y = -colcutoff, color = blackc, linestyle = 'dashed')\n",
    "                plt.axhline(y = 0.75*colcutoff, color = blackc, linestyle = 'dotted') #additional lines\n",
    "                plt.axhline(y = -0.75*colcutoff, color = blackc, linestyle = 'dotted')\n",
    "            \n",
    "            #print('sequence')\n",
    "            #print(subset.index.get_level_values('Sequence'))\n",
    "            for peptide in pept.index.get_level_values('Sequence').unique():\n",
    "                if peptide in subset.index:\n",
    "                    # Extract mean values for current peptide\n",
    "                    mean_values = subset.loc[peptide].values\n",
    "                    #print('mean values')\n",
    "                    #print(mean_values)\n",
    "                    # Extract start and end values\n",
    "                    start_end_values = pept.loc[peptide, ['Start', 'End']].values\n",
    "                    starts = start_end_values[0]\n",
    "                    #print('starts')\n",
    "                    #print(starts)\n",
    "                    ends = start_end_values[1]\n",
    "                    #print('ends')\n",
    "                    #print(ends)\n",
    "                    # Update first_start and last_end values\n",
    "                    startsl = []\n",
    "                    endsl = []\n",
    "                    startsl.append(start_end_values[0])  # Access 'Start' column directly\n",
    "                    endsl.append(start_end_values[1])      # Access 'End' column directly\n",
    "                    if first_start is None:\n",
    "                        first_start = startsl[0]\n",
    "                    if last_end is None or last_end < endsl[-1]:\n",
    "                        last_end = endsl[-1]\n",
    "                    # Plot the line for current peptide\n",
    "                    if not pd.isna(mean_values).all():\n",
    "                        mean_value = mean_values[0]\n",
    "                        if color_by_heatmap == 1:\n",
    "                            cmap = colormap\n",
    "                            norm = my_norm\n",
    "                            color = cmap(norm(mean_value))\n",
    "                        elif mean_values > colcutoff:\n",
    "                            color = woodscolpos\n",
    "                        elif mean_values < -colcutoff:\n",
    "                            color = woodscolneg\n",
    "                        else: \n",
    "                            color = woodscolneu\n",
    "                        plt.plot([starts, ends], [mean_value, mean_value], label=f\"{peptide}\", color=color, lw=woodscolthick, path_effects=[pe.Stroke(linewidth=woodscolthick+woodsbackthick, foreground='black'), pe.Normal()])\n",
    "                    else:\n",
    "                        print(f\"Skipping plotting for peptide {peptide}: data length mismatch.\")\n",
    "            # Set plot title and labels\n",
    "            plt.title(f\"{title} - State Comparison: {state}, Exposure: {exposure}\", pad=wpadthick)\n",
    "            plt.xlabel('Position')\n",
    "            plt.ylabel('Change in Deuterium Uptake (Da)')\n",
    "            ax = plt.gca()\n",
    "            if first_start-(last_end*1/20) <= 0:\n",
    "                ax.set_xlim([0, last_end+(last_end*1/20)])\n",
    "            else:\n",
    "                ax.set_xlim([first_start-(last_end*1/20), last_end+(last_end*1/20)])\n",
    "            ax.set_ylim([-max_delta-(max_delta/10), max_delta+(max_delta/10)])\n",
    "            for section in color_sections:\n",
    "                    start, end, color, text = section\n",
    "                    # Adjust y position of the rectangle and annotation to be above the top ticks\n",
    "                    rect_y = blockposwoods  # Position above the top of the heatmap\n",
    "                    rect = plt.Rectangle((start+1, rect_y), end-start, blockthickwoods, color=color, clip_on=False)\n",
    "                    ax.add_patch(rect)\n",
    "                    text_y = blocktextposwoods + rect_y + blockthickwoods / 2 \n",
    "                    ax.annotate(text, xy=((start+end)/2, text_y), xytext=(0,0), textcoords='offset points',\n",
    "                                ha='center', va='center', fontsize=20, color=fontcolor, clip_on=False, annotation_clip=False)\n",
    "                    print('adding section')\n",
    "            #plt.legend()\n",
    "            plt.grid(True)\n",
    "            # Save the plot to the PDF\n",
    "            pdf.savefig(bbox_inches='tight')\n",
    "            # Save the plot as an image file\n",
    "            img_buffer = BytesIO()\n",
    "            if PDFgeneration == 1:\n",
    "                plt.savefig(img_buffer, format='pdf', dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "            else:\n",
    "                plt.savefig(img_buffer, format='png', dpi=output_bitmap_dpi, bbox_inches='tight')\n",
    "            img_buffer.seek(0)\n",
    "            # Add the image file to the zip archive\n",
    "            if PDFgeneration == 1:\n",
    "                zip_file.writestr(f\"{title}_State_{state}_Exposure_{exposure}.pdf\", img_buffer.read())\n",
    "            else:\n",
    "                zip_file.writestr(f\"{title}_State_{state}_Exposure_{exposure}.png\", img_buffer.read())\n",
    "            plt.close()\n",
    "    \n",
    "    \n",
    "    if plot_w:\n",
    "        all_deltas_grouped = all_deltas.groupby('Protein')\n",
    "        # Create a ZipFile object to store the plots\n",
    "        output_buffer = BytesIO()\n",
    "        with zipfile.ZipFile(output_buffer, 'w', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "            for prot, data_subset in all_deltas_grouped:\n",
    "                for state in all_states:\n",
    "                    data = data_subset.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                    pept = data_subset.loc[:,['Start','End']]\n",
    "                    title = prot\n",
    "                    w_plot(pdf,data,title,pept,state,data_avg,max_delta,colcutoff)\n",
    "        output_buffer.seek(0)\n",
    "        #zip_filename = generate_unique_filename('Plots', 'zip', timestamp)\n",
    "        #upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), zip_filename)\n",
    "    elif split_outp_by_prot != None:    \n",
    "        if split_outp_by_prot == 'all':\n",
    "            all_deltas_grouped = all_deltas.groupby('Protein')\n",
    "            for prot, data_subset in all_deltas_grouped:\n",
    "                if plot_stacked != None:\n",
    "                    if plot_stacked:\n",
    "                        data = data_subset.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "                        pept = data_subset.loc[:,['Start','End']]\n",
    "                        if plot_h:\n",
    "                            title = 'D uptake difference; Protein: ' + prot + '\\nTop to bottom: ' + ', '.join(all_states.to_list())\n",
    "                            h_plot(pdf, data, title, pept, color_sections)\n",
    "                        if plot_v:\n",
    "                            title = 'D uptake difference; Protein: ' + prot + '\\nLeft to right: ' + ', '.join(all_states.to_list())\n",
    "                            v_plot(pdf, data, title, pept)\n",
    "                if plot_separate != None:\n",
    "                    if plot_separate:\n",
    "                        for state in all_states:\n",
    "                            data = data_subset.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                            pept = data_subset.loc[:,['Start','End']]\n",
    "                            title = 'D uptake difference: ' + state + '\\nProtein: ' + prot\n",
    "                            if plot_h:\n",
    "                                h_plot(pdf, data, title, pept, color_sections)\n",
    "                            if plot_v:\n",
    "                                v_plot(pdf, data, title, pept)\n",
    "    elif type(split_outp_by_prot) == str and len(split_outp_by_prot) > 0:\n",
    "        print(\"Variable split_outp_by_prot is set to an unrecognized string '%s'. Either make split_outp_by_prot a list of protein IDs, make it split_outp_by_prot = 'all' to split each protein into a separate plot, or leave it as an empty string (default value) to plot data without splitting by protein ID. No plots will be produced with current settings.\\n\" % split_outp_by_prot)\n",
    "    elif type(split_outp_by_prot) == list and len(split_outp_by_prot) > 0:\n",
    "        all_deltas_grouped = all_deltas.groupby('Protein')\n",
    "        for prot in split_outp_by_prot:\n",
    "            if type(prot) == tuple:\n",
    "                data_subset = pd.concat([all_deltas_grouped.get_group(key) for key in prot])\n",
    "                prot_string = '; Proteins: ' + ' '.join(prot)\n",
    "            else:\n",
    "                data_subset = all_deltas_grouped.get_group(prot)\n",
    "                prot_string = '; Protein: ' + prot\n",
    "            if plot_stacke == 1:\n",
    "                data = data_subset.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "                pept = data_subset.loc[:,['Start','End']]\n",
    "                if plot_h:\n",
    "                    title = 'D uptake difference' + prot_string + '\\nTop to bottom: ' + ', '.join(all_states.to_list())\n",
    "                    h_plot(pdf, data, title, pept, color_sections)\n",
    "                if plot_v:\n",
    "                    title = 'D uptake difference' + prot_string + '\\nLeft to right: ' + ', '.join(all_states.to_list())\n",
    "                    v_plot(pdf, data, title, pept)\n",
    "            elif plot_separate == 1:\n",
    "                for state in all_states:\n",
    "                    data = data_subset.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                    pept = data_subset.loc[:,['Start','End']]\n",
    "                    title = 'D uptake difference: ' + state + prot_string\n",
    "                    if plot_h:\n",
    "                        h_plot(pdf, data, title, pept, color_sections)\n",
    "                    if plot_v:\n",
    "                        v_plot(pdf, data, title, pept)\n",
    "    elif split_outp_chunks:\n",
    "        chunk_bounds = np.ceil(np.linspace(0, all_deltas.shape[0], split_outp_chunks+1)).astype(int)\n",
    "        for i in range(split_outp_chunks):\n",
    "            data_subset = all_deltas.iloc[chunk_bounds[i]:chunk_bounds[i+1]]\n",
    "            if plot_stacked == 1:\n",
    "                data = data_subset.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "                pept = data_subset.loc[:,['Start','End']]\n",
    "                if plot_h:\n",
    "                    title = 'D uptake difference\\nTop to bottom: ' + ', '.join(all_states.to_list())\n",
    "                    h_plot(pdf, data, title, pept, color_sections)\n",
    "                if plot_v:\n",
    "                    title = 'D uptake difference\\nLeft to right: ' + ', '.join(all_states.to_list())\n",
    "                    v_plot(pdf, data, title, pept)\n",
    "            elif plot_separate == 1:\n",
    "                for state in all_states:\n",
    "                    data = data_subset.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                    title = 'D uptake difference: ' + state\n",
    "                    pept = data_subset.loc[:,['Start','End']]\n",
    "                    if plot_h:\n",
    "                        h_plot(pdf, data, title, pept, color_sections)\n",
    "                    if plot_v:\n",
    "                        v_plot(pdf, data, title, pept)\n",
    "    elif split_outp_by_row:\n",
    "        split_outp_by_row = [0] + split_outp_by_row + [all_deltas.shape[0]]\n",
    "        for i in range(len(split_outp_by_row)-1):\n",
    "            data_subset = all_deltas.iloc[split_outp_by_row[i]:split_outp_by_row[i+1]]\n",
    "            if plot_stacked == 1:\n",
    "                data = data_subset.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "                pept = data_subset.loc[:,['Start','End']]\n",
    "                if plot_h:\n",
    "                    title = 'D uptake difference\\nTop to bottom: ' + ', '.join(all_states.to_list())\n",
    "                    h_plot(pdf, data, title, pept, color_sections)\n",
    "                if plot_v:\n",
    "                    title = 'D uptake difference\\nLeft to right: ' + ', '.join(all_states.to_list())\n",
    "                    v_plot(pdf, data, title, pept)\n",
    "            elif plot_separate == 1:\n",
    "                for state in all_states:\n",
    "                    data = data_subset.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                    title = 'D uptake difference: ' + state\n",
    "                    pept = data_subset.loc[:,['Start','End']]\n",
    "                    if plot_h:\n",
    "                        h_plot(pdf, data, title, pept, color_sections)\n",
    "                    if plot_v:\n",
    "                        v_plot(pdf, data, title, pept)\n",
    "    else:\n",
    "        if plot_stacked == 1:\n",
    "            data = all_deltas.loc[:,('Delta_d_uptake','mean',slice(None),slice(None))]\n",
    "            pept = all_deltas.loc[:,['Start','End']]\n",
    "            if plot_h:\n",
    "                title = 'D uptake difference\\nTop to bottom: ' + ', '.join(all_states.to_list())\n",
    "                h_plot(pdf, data, title, pept, color_sections)\n",
    "            if plot_v:\n",
    "                title = 'D uptake difference\\nLeft to right: ' + ', '.join(all_states.to_list())\n",
    "                v_plot(pdf, data, title, pept)\n",
    "        elif plot_separate == 1:\n",
    "            for state in all_states:\n",
    "                data = all_deltas.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                title = 'D uptake difference: ' + state\n",
    "                pept = all_deltas.loc[:,['Start','End']]\n",
    "                if plot_h:\n",
    "                    h_plot(pdf, data, title, pept, color_sections)\n",
    "                if plot_v:\n",
    "                    v_plot(pdf, data, title, pept)\n",
    "\n",
    "\n",
    "    pdf.close()\n",
    "    \n",
    "    ################################################################################################\n",
    "    download_pymol = int(request.form.get('download_pymol', 0))\n",
    "    output_files = []\n",
    "    h_or_v = float(request.form['h_or_v'])\n",
    "    if h_or_v == 1:\n",
    "        plot_v = 0\n",
    "        plot_h = 1\n",
    "        plot_w = 0\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 2:\n",
    "        plot_v = 1\n",
    "        plot_h = 0\n",
    "        plot_w = 0\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 3:\n",
    "        plot_v = 0\n",
    "        plot_h = 0\n",
    "        plot_w = 1\n",
    "        plot_volc = 0\n",
    "    elif h_or_v == 4:\n",
    "        plot_v = 0\n",
    "        plot_h = 0\n",
    "        plot_w = 0\n",
    "        plot_volc = 1\n",
    "    if plot_volc == 1:\n",
    "        scatter_plot = 1\n",
    "    elif plot_volc == 0:\n",
    "        scatter_plot = 2020\n",
    "    \n",
    "    def download_collect(chain_dict, chain_id, colors, bounds, all_deltas, all_states, buffer, heatmap_buffer, output_buffer, mk_pymol):\n",
    "        h_or_v = float(request.form['h_or_v'])\n",
    "        pymol_dir = 'pymol_macros'\n",
    "        Path(pymol_dir).mkdir(parents=True, exist_ok=True)\n",
    "        if chain_dict:\n",
    "            if chain_id:\n",
    "                print('Both variables chain_dict and chain_id are not empty; Using values from chain_dict, which overrides chain_id.\\n')\n",
    "        else:\n",
    "            for prot in all_deltas['Protein'].unique():\n",
    "                chain_dict[prot] = chain_id\n",
    "        pymol_header = \"alter all, b=1000\\ncolor %s, all\\n\\n\" %  ('0x' + mc.to_hex(c_missing)[1:])\n",
    "        curr_color = '0x' + mc.to_hex(colors[0])[1:]\n",
    "        pymol_footer = \"\\n\\ncolor %s, b<%.3f\\n\" % (curr_color, bounds[0])\n",
    "        for i in range(len(bounds)-1):\n",
    "            curr_color = '0x' + mc.to_hex(colors[i])[1:]\n",
    "            pymol_footer += \"color %s, (b>%.3f or b=%.3f) and b<%.3f\\n\" % (curr_color, bounds[i], bounds[i], bounds[i+1])\n",
    "        curr_color = '0x' + mc.to_hex(colors[-1])[1:]\n",
    "        pymol_footer += \"color %s, (b>%.3f or b=%.3f) and b<999\\n\" % (curr_color, bounds[-1], bounds[-1])\n",
    "        for prot, data_subset in all_deltas_grouped:    \n",
    "            for state in all_states:\n",
    "                data = all_deltas.loc[:,('Delta_d_uptake','mean',state,slice(None))]\n",
    "                time_list = data.columns.get_level_values('Exposure').unique()\n",
    "                for i in time_list:\n",
    "                    mask = data.loc[:,('Delta_d_uptake', 'mean', state, i)].notna().to_list()\n",
    "                    pymol_str = pd.Series(list(map(mk_pymol, all_deltas['Protein'], all_deltas['Start'], all_deltas['End'], data.loc[:,('Delta_d_uptake', 'mean', state, i)])))[mask]\n",
    "                    pymol_script = pymol_str.to_string(header=False, index=False)\n",
    "                    file_name = \"%s_%s_%s.pml\" % (prot, state, str(i))\n",
    "                    if pymol_dir:\n",
    "                        file_name = pymol_dir + \"/\" + file_name\n",
    "                    f_pymol = open(file_name, 'w')\n",
    "                    print(pymol_header, pymol_script, pymol_footer, file=f_pymol)\n",
    "                    f_pymol.close()\n",
    "                    # Append the file name to the list\n",
    "                    output_files.append(file_name)\n",
    "        # Create a zip file\n",
    "        zip_file_name = 'output_files.zip'\n",
    "        with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "            for file in output_files:\n",
    "                zipf.write(file, os.path.basename(file))\n",
    "        # Create a zip file containing the Pymol zip and heatmap.png\n",
    "        combined_buffer = io.BytesIO()\n",
    "        print(f\"h_or_v value: {h_or_v}\")\n",
    "        with zipfile.ZipFile(combined_buffer, 'w') as zipf:\n",
    "            zipf.write(zip_file_name, os.path.basename(zip_file_name))\n",
    "            if h_or_v == 3:\n",
    "                zipf.writestr('WoodsPlots.zip', output_buffer.getbuffer())\n",
    "            elif len(all_prot) > 1:\n",
    "                zipf.writestr('heatmaps.zip', heatmap_buffer.getbuffer())\n",
    "            elif PDFgeneration == 1:\n",
    "                zipf.writestr('heatmap.zip', heatmap_buffer.getbuffer())\n",
    "            else:\n",
    "                zipf.writestr('heatmap.zip', heatmap_buffer.getbuffer())\n",
    "        # Set the cursor to the beginning of the combined buffer\n",
    "        combined_buffer.seek(0)\n",
    "        # Send the combined zip file as a response\n",
    "        print('pymol w plot')\n",
    "        response = make_response(send_file(combined_buffer, as_attachment=True, download_name='plot_and_pymol.zip'))\n",
    "        response.headers['Content-Disposition'] = 'attachment; filename=plot_and_pymol.zip'\n",
    "        response.headers['Content-Type'] = 'application/zip'\n",
    "        return response\n",
    "\n",
    "    if scatter_plot != 2020:\n",
    "        scatter_zip_name = generate_unique_filename('scatter_plots', 'zip', timestamp)\n",
    "        scatter_blob_name = upload_to_blob_storage(container_client, scatter_buffer.getvalue(), scatter_zip_name)\n",
    "        scatter_blob_stream = download_blob_as_bytes(container_client, scatter_blob_name)\n",
    "        response = make_response(send_file(scatter_blob_stream, as_attachment=True, download_name=scatter_zip_name))\n",
    "        response.headers['Content-Type'] = 'application/zip'\n",
    "        response.headers['Content-Disposition'] = f'attachment; filename={scatter_zip_name}'\n",
    "        print('scatterplot')\n",
    "        return response\n",
    "    else:\n",
    "        if download_pymol == 1:\n",
    "            print('pymol')\n",
    "            return download_collect(chain_dict, chain_id, colors, bounds, all_deltas, all_states, buffer, heatmap_buffer, output_buffer, mk_pymol)\n",
    "            # Assuming download_collect is modified to return a buffer\n",
    "            # pymol_buffer = download_collect(chain_dict, chain_id, colors, bounds, all_deltas, all_states, buffer, mk_pymol)\n",
    "            # pymol_filename = generate_unique_filename('plot_and_pymol', 'zip', timestamp)\n",
    "            # pymol_blob_name = upload_to_blob_storage(container_client, pymol_buffer.getvalue(), pymol_filename)\n",
    "            # if pymol_blob_name:\n",
    "            #     pymol_blob_stream = download_blob_as_bytes(container_client, pymol_blob_name)\n",
    "            #     response = make_response(send_file(pymol_blob_stream, as_attachment=True, download_name=pymol_filename))\n",
    "            #     response.headers['Content-Type'] = 'application/zip'\n",
    "            #     response.headers['Content-Disposition'] = f'attachment; filename={pymol_filename}'\n",
    "            #     return response\n",
    "            # else:\n",
    "            #     return \"Failed to upload Pymol files to Azure Blob Storage\"\n",
    "        elif plot_w != 0:\n",
    "            zip_filename = generate_unique_filename('WoodsPlots', 'zip', timestamp)\n",
    "            # pdf_filename = generate_unique_filename('WoodsPlots', 'pdf', timestamp)\n",
    "            # if PDFgeneration == 1:\n",
    "            #     upload_to_blob_storage(container_client, output_buffer, pdf_filename) \n",
    "            #     pdf_blob_stream = download_blob_as_bytes(container_client, pdf_filename)\n",
    "            #     response = make_response(send_file(pdf_blob_stream, as_attachment=True, download_name=pdf_filename))\n",
    "            #     response.headers['Content-Type'] = 'application/pdf'\n",
    "            #     response.headers['Content-Disposition'] = f'attachment; filename={pdf_filename}'\n",
    "            #     return response\n",
    "            # else:\n",
    "            upload_to_blob_storage(container_client, output_buffer, zip_filename)\n",
    "            zip_blob_stream = download_blob_as_bytes(container_client, zip_filename)\n",
    "            response = make_response(send_file(zip_blob_stream, as_attachment=True, download_name=zip_filename))\n",
    "            response.headers['Content-Type'] = 'application/zip'\n",
    "            response.headers['Content-Disposition'] = f'attachment; filename={zip_filename}'\n",
    "            output_buffer.close()\n",
    "            print('woods')\n",
    "            return response\n",
    "        elif len(all_prot) > 1:\n",
    "            heatmaps_zip_name = generate_unique_filename('heatmaps', 'zip', timestamp)\n",
    "            upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), heatmaps_zip_name)\n",
    "            zip_blob_stream = download_blob_as_bytes(container_client, heatmaps_zip_name)\n",
    "            response = make_response(send_file(zip_blob_stream, as_attachment=True, download_name=heatmaps_zip_name))\n",
    "            response.headers['Content-Type'] = 'application/zip'\n",
    "            response.headers['Content-Disposition'] = f'attachment; filename={heatmaps_zip_name}'\n",
    "            print('1 heatmap')\n",
    "            return response\n",
    "        elif PDFgeneration == 1:\n",
    "            heatmaps_zip_name = generate_unique_filename('heatmap', 'zip', timestamp)\n",
    "            upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), heatmaps_zip_name)\n",
    "            zip_blob_stream = download_blob_as_bytes(container_client, heatmaps_zip_name)\n",
    "            response = make_response(send_file(zip_blob_stream, as_attachment=True, download_name=heatmaps_zip_name))\n",
    "            response.headers['Content-Type'] = 'application/zip'\n",
    "            response.headers['Content-Disposition'] = f'attachment; filename={heatmaps_zip_name}'\n",
    "            print('pdf')\n",
    "            return response\n",
    "        else:\n",
    "            heatmaps_zip_name = generate_unique_filename('heatmap', 'zip', timestamp)\n",
    "            upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), heatmaps_zip_name)\n",
    "            zip_blob_stream = download_blob_as_bytes(container_client, heatmaps_zip_name)\n",
    "            response = make_response(send_file(zip_blob_stream, as_attachment=True, download_name=heatmaps_zip_name))\n",
    "            response.headers['Content-Type'] = 'application/zip'\n",
    "            response.headers['Content-Disposition'] = f'attachment; filename={heatmaps_zip_name}'\n",
    "            print('all else')\n",
    "            return response\n",
    "            #if PDFgeneration == 1:\n",
    "            #    if plot_h:\n",
    "            #        hvtitle = '_h'\n",
    "            #    else:\n",
    "            #        hvtitle = '_v'\n",
    "            #    pdf_filename = generate_unique_filename(output_bitmap_name + hvtitle, 'pdf', timestamp)\n",
    "            #    upload_to_blob_storage(container_client, buffer.getvalue(), pdf_filename)\n",
    "            #    pdf_blob_stream = download_blob_as_bytes(container_client, pdf_filename)\n",
    "            #    response = make_response(send_file(pdf_blob_stream, as_attachment=True, download_name=pdf_filename))\n",
    "            #    return response\n",
    "                #else:\n",
    "                    #return \"Failed to upload heatmap PDF to Azure Blob Storage\"\n",
    "            #else:\n",
    "            #    if len(all_prot) > 1:\n",
    "            #        heatmaps_zip_name = generate_unique_filename('heatmaps', 'zip',timestamp)\n",
    "            #        heatmaps_zip_blob_name = upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), heatmaps_zip_name)\n",
    "            #        if heatmaps_zip_blob_name:\n",
    "            #            heatmaps_zip_blob_stream = download_blob_as_bytes(container_client, heatmaps_zip_blob_name)\n",
    "            #            response = make_response(send_file(heatmaps_zip_blob_stream, as_attachment=True, download_name=heatmaps_zip_name))\n",
    "            #            return response\n",
    "            #        else:\n",
    "            #            return \"Failed to upload heatmaps zip to Azure Blob Storage\"\n",
    "            #    else:\n",
    "            #        heatmap_png_name = generate_unique_filename('heatmap', 'png', timestamp)\n",
    "            #        heatmap_png_blob_name = upload_to_blob_storage(container_client, heatmap_buffer.getvalue(), heatmap_png_name)\n",
    "            #        if heatmap_png_blob_name:\n",
    "            #            heatmap_png_blob_stream = download_blob_as_bytes(container_client, heatmap_png_blob_name)\n",
    "            #            response = make_response(send_file(heatmap_png_blob_stream, as_attachment=True, download_name=heatmap_png_name))\n",
    "            #            return response\n",
    "            #        else:\n",
    "            #            return \"Failed to upload heatmap PNG to Azure Blob Storage\"    \n",
    "\n",
    "@app.route('/reset_values', methods=['POST'])\n",
    "def reset_values():\n",
    "    return render_template('index.html', output=None)\n",
    "\n",
    "\n",
    "@app.route('/help_doc', methods=['Post'])\n",
    "def help_doc():\n",
    "    AZURE_STORAGE_CONNECTION_STRINGhd = \"\"\n",
    "    CONTAINER_NAMEhd = \"uploads\"\n",
    "    BLOB_NAMEhd = 'HDgraphiX Help Doc.pdf'\n",
    "    blob_service_clienthd = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRINGhd)\n",
    "    blob_clienthd = blob_service_clienthd.get_blob_client(container=CONTAINER_NAMEhd, blob=BLOB_NAMEhd)\n",
    "    blob_datahd = blob_clienthd.download_blob()\n",
    "    blob_contenthd = io.BytesIO(blob_datahd.readall())\n",
    "\n",
    "    # Send the file to the user\n",
    "    return send_file(\n",
    "        blob_contenthd,\n",
    "        as_attachment=True,\n",
    "        download_name=BLOB_NAMEhd)\n",
    "\n",
    "@app.route('/sample_file', methods=['Post'])\n",
    "def sample_file():\n",
    "    AZURE_STORAGE_CONNECTION_STRINGsf = \"\"\n",
    "    CONTAINER_NAMEsf = \"uploads\"\n",
    "    BLOB_NAMEsf = 'HDgraphiX Sample DynamX.csv'\n",
    "    blob_service_clientsf = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRINGsf)\n",
    "    blob_clientsf = blob_service_clientsf.get_blob_client(container=CONTAINER_NAMEsf, blob=BLOB_NAMEsf)\n",
    "    blob_datasf = blob_clientsf.download_blob()\n",
    "    blob_contentsf = io.BytesIO(blob_datasf.readall())\n",
    "\n",
    "    # Send the file to the user\n",
    "    return send_file(\n",
    "        blob_contentsf,\n",
    "        as_attachment=True,\n",
    "        download_name=BLOB_NAMEsf)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=8000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6e7e2-d111-473f-b8af-b0de101cfd63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7935121",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7d798-158c-401d-ae5e-8cd567af2f65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea60e20a-c004-45c2-9795-3c949cf22f2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
